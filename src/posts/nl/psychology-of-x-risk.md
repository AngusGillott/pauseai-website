---
title: De moeilijke psychologie van existentiële risico's
description: Denken aan het einde van de wereld is moeilijk.
---

De meeste mensen reageren aanvankelijk op het onderwerp van AI-existentiële risico's met een mix van spot, ontkenning en ongeloof.
De angst komt vaak pas na een lange tijd van erover nadenken.

De psychologie van existentiële risico's is een onderwerp dat niet vaak wordt besproken, vergeleken met de technische aspecten van AI-veiligheid.
Toch zou men kunnen beargumenteren dat het misschien net zo belangrijk is.
Immers, als we mensen niet kunnen overtuigen om het onderwerp serieus te nemen en er actie op te ondernemen, kunnen we er niets aan doen.

Het is moeilijk om het te **aansnijden**, moeilijk om te **geloven**, moeilijk om te **begrijpen**, en moeilijk om er **actie op te ondernemen**.
Een beter begrip van _waarom_ deze dingen zo moeilijk zijn, kan ons helpen overtuigender, effectiever en empathischer te zijn.

## Moeilijk om aan te snijden

X-risk is een moeilijk onderwerp om in een gesprek aan te snijden, vooral als je een politicus bent.
Mensen kunnen denken dat je gek bent, en je voelt je misschien niet op je gemak om over dit technisch complexe onderwerp te praten.

### Angst om belachelijk gemaakt te worden

De eerste reactie op existentiële risico's is vaak gewoon het weglachen ervan.
We hebben dit ook op camera gezien in het Witte Huis, de eerste keer dat x-risk ter sprake kwam.
Dit maakt het op zijn beurt moeilijker om het onderwerp opnieuw aan te snijden, omdat anderen bang zullen zijn om belachelijk gemaakt te worden voor het ter sprake brengen.

Professionals kunnen vrezen dat hun reputatie wordt geschaad als ze hun zorgen delen.

> “Het was bijna gevaarlijk vanuit een carrièreperspectief om toe te geven dat je je zorgen maakte,” - [zei Jeff Clune](https://www.theglobeandmail.com/business/article-i-hope-im-wrong-why-some-experts-see-doom-in-ai/)

Drukken voor verstandige beleidsmaatregelen (zoals een pauze) kan als "extremistisch" of "alarmistisch" worden beschouwd, wat beide je geloofwaardigheid of reputatie kan schaden.

### Angst om als racist/cultist/samenzweerder te worden bestempeld

In de afgelopen maanden zijn verschillende samenzweringstheorieën opgekomen.
Sommige individuen hebben verklaard dat [alle AI-veiligheidsmensen racistisch zijn](https://medium.com/%2540emilymenonbender/talking-about-a-schism-is-ahistorical-3c454a77220f) en dat [AI-veiligheid een cult is](https://www.cnbc.com/2023/06/06/ai-doomers-are-a-cult-heres-the-real-threat-says-marc-andreessen.html).
Sommigen hebben verklaard dat AI 'doomers' deel uitmaken van een [samenzwering van grote technologie om AI "op te hypen"](https://www.latimes.com/business/technology/story/2023-03-31/column-afraid-of-ai-the-startups-selling-it-want-you-to-be).
Deze belachelijke beschuldigingen kunnen bezorgde mensen terughoudend maken om hun zorgen te delen.

Echter, voordat je boos wordt op de mensen die deze beschuldigingen maken, houd er rekening mee dat ze het resultaat kunnen zijn van angst en ontkenning (zie hieronder).
Het erkennen van de gevaren van AI is eng, en het kan gemakkelijker zijn om de boodschapper te verwerpen dan de boodschap te internaliseren.

### Een complex onderwerp om over te discussiëren

Mensen praten graag over dingen waar ze kennis van hebben.
De technische moeilijkheid van AI-veiligheid maakt het een ontmoedigend onderwerp voor de meeste mensen.
Het kost tijd en moeite om de argumenten te begrijpen.
Als politicus wil je niet betrapt worden op het zeggen van iets dat verkeerd is, dus je zou het onderwerp misschien helemaal vermijden.

## Moeilijk om te geloven

Zelfs als er een discussie is over existentiële risico's, is het moeilijk om mensen ervan te overtuigen dat het een echt probleem is.
Er zijn verschillende redenen waarom de meesten het idee onmiddellijk zullen afwijzen.

### Normalcy Bias

We kennen allemaal de beelden van rampen in films, toch? Mensen die schreeuwen en in paniek wegrennen.
Het blijkt dat het tegenovergestelde vaak waar is: ongeveer 80% van de mensen vertoont symptomen van [_normalcy bias_](https://en.wikipedia.org/wiki/Normalcy_bias) tijdens rampen: geen schuilplaats zoeken tijdens een tornado, overheidswaarschuwingen negeren, blijven handen schudden in de vroege COVID-dagen.
De normalcy bias beschrijft onze neiging om de mogelijkheid van een ramp te onderschatten en te geloven dat het leven normaal zal doorgaan, zelfs in het licht van significante bedreigingen of crises.

> Mensen draaien rond, vragen om meningen, omdat ze _willen_ horen dat alles goed is. Ze blijven vragen en uitstellen totdat ze het antwoord krijgen dat ze willen.

> Tijdens 9/11 was bijvoorbeeld de gemiddelde wachttijd onder overlevenden om de torens te evacueren 6 minuten, waarbij sommigen tot een half uur wachtten om te vertrekken. Ongeveer 1000 mensen namen zelfs de tijd om hun computers uit te schakelen en andere kantooractiviteiten te voltooien, een strategie om door te gaan met normale activiteiten tijdens een onbekende situatie.

_Van ["The frozen calm of normalcy bias"](https://gizmodo.com/the-frozen-calm-of-normalcy-bias-486764924)_

Een ander voorbeeld hiervan is de Challenger-space shuttle-ramp in 1986.
Roger Boisjoly was een ingenieur die voorspelde dat het zou ontploffen, maar niemand van zijn managers wilde geloven dat het mogelijk was:

> We wisten allemaal dat als de afdichtingen faalden, de shuttle zou ontploffen.
> Ik heb als een gek gevochten om die lancering te stoppen. Ik ben zo van binnen verscheurd dat ik er zelfs nu nauwelijks over kan praten.
> We spraken met de juiste mensen, we spraken met de mensen die de macht hadden om die lancering te stoppen.

_Van ["Remembering Roger Boisjoly"](https://www.npr.org/sections/thetwo-way/2012/02/06/146490064/remembering-roger-boisjoly-he-tried-to-stop-shuttle-challenger-launch)_

Een verklaring voor waarom onze hersenen weigeren te geloven dat gevaar ons te wachten staat, is cognitieve dissonantie.

### Cognitieve dissonantie

Wanneer we worden geconfronteerd met nieuwe informatie, probeert de hersenen deze te laten passen bij wat we al weten.
Ideeën die al overeenkomen met bestaande overtuigingen worden gemakkelijk toegevoegd aan ons wereldmodel.
Ideeën die te verschillend zijn van wat we al geloven, veroorzaken _cognitieve dissonantie_ - we zullen ons ongemakkelijk voelen en proberen de ideeën te verwerpen of alternatieve verklaringen te vinden voor wat we horen.

Veel overtuigingen die de meeste mensen hebben, zullen worden uitgedaagd door het idee van existentiële risico's:

- Technologie is er om ons te dienen en kan gemakkelijk worden gecontroleerd
- Er zijn slimme mensen aan de macht die ervoor zullen zorgen dat alles goed komt
- Ik zal waarschijnlijk oud worden, en dat zullen mijn kinderen ook

Veel van deze gedachten zullen worden uitgedaagd door het idee dat AI een existentiëel risico vormt.
Onze hersenen zullen op zoek gaan naar alternatieve verklaringen voor waarom ze horen dat wetenschappers ons hiervoor waarschuwen:

- Ze worden betaald door grote technologie
- Ze maken deel uit van een of andere samenzwering of cult
- Ze willen gewoon aandacht of macht

Het internaliseren van het idee dat _wetenschappers ons waarschuwen omdat ze geloven dat we in gevaar zijn_ botst met onze bestaande overtuigingen, het veroorzaakt te veel cognitieve dissonantie.

### Het einde van de wereld is nog nooit gebeurd

Zien is geloven (zie: [_Availability Heuristic_](https://en.wikipedia.org/wiki/Availability_heuristic)).
Dat is een probleem voor het risico op uitsterven, omdat we het nooit zullen kunnen zien voordat het te laat is.

Aan de andere kant hebben we tonnen bewijs voor het tegendeel.
Het einde der tijden is door veel mensen voorspeld, en elke enkele voorspelling is tot nu toe verkeerd gebleken.

Dus wanneer mensen horen over existentiële risico's, denken ze dat het gewoon weer een van die doomsday cult voorspellingen is.
Probeer begrip te hebben voor dit standpunt, en wees niet te hard voor mensen die zo denken.
Ze hebben waarschijnlijk niet dezelfde informatie gezien als jij.

### We denken graag dat we speciaal zijn

Zowel op een _collectief_ als op een _individueel_ niveau willen we geloven dat we speciaal zijn.

Op collectief niveau denken we graag dat mensen iets heel anders zijn dan dieren - Darwin's idee dat we van apen zijn geëvolueerd was voor de meesten bijna ondenkbaar.
De meeste religies hebben verhalen over de hemel of reïncarnatie, waar mensen (of in ieder geval de gelovigen) op de een of andere manier voor altijd zullen leven.
Het idee dat de mensheid op een dag misschien niet meer zal bestaan, is zeer schokkend en moeilijk te internaliseren.
We willen geloven dat we _plot armor_ hebben - dat we de hoofdpersonen in een verhaal zijn, en dat het verhaal een gelukkig einde zal hebben.
Mensen kunnen het rationeel overwegen, maar ze zullen het niet _voelen_.
Een video van Robert Miles getiteld ["There's no rule which says we will make it"](https://www.youtube.com/watch?v=JD_iA7imAPs) legt dit heel goed uit.

Op individueel niveau zijn we trots op de unieke intellectuele vaardigheden die we hebben.
Velen wilden nooit geloven dat een AI op een dag in staat zou zijn om kunst te creëren, boeken te schrijven of zelfs beter te zijn in schaken dan wij.
De gedachte dat onze eigen intelligentie slechts een product van evolutie is en dat het door een machine kan worden gerepliceerd, is iets dat veel mensen moeilijk kunnen accepteren.
Dit maakt het moeilijk om te accepteren dat een AI intelligenter zou kunnen zijn dan wij.

### Fictie heeft ons geconditioneerd om een gelukkig einde te verwachten

Het meeste wat we weten over existentiële risico's komt uit fictie.
Dit helpt waarschijnlijk niet, omdat fictieve verhalen niet zijn geschreven om realistisch te zijn: ze zijn geschreven om vermakelijk te zijn.

In fictie is er vaak een held, conflict, hoop en uiteindelijk een gelukkig einde.
We zijn geconditioneerd om een strijd te verwachten waarin we kunnen vechten en winnen.
In sci-fi worden AIs vaak zeer antropomorfisch afgebeeld - als kwaad, als willen ze menselijk zijn, als veranderen ze hun doelen.
Dit komt allemaal niet overeen met waar AI-veiligheidsexperts zich zorgen over maken.

En in de meeste verhalen wint de held.
De AI maakt een domme fout en de held vindt een manier om het ding dat supposed to be veel slimmer is, te slim af te zijn.
De held wordt beschermd door plot armor.
In meer realistische AI-doemscenario's is er geen held, geen plot armor, geen strijd, geen mensen die een superintelligentie slim af zijn, en geen gelukkig einde.

### Vooruitgang is altijd (meestal) goed geweest

Veel van de technologieën die in onze samenleving zijn geïntroduceerd, zijn grotendeels voordelig geweest voor de mensheid.
We hebben ziektes genezen, onze levensverwachting verhoogd en ons leven comfortabeler gemaakt.
En elke keer dat we dat deden, waren er mensen die zich tegen deze innovaties verzetten en waarschuwden voor de gevaren.
De Luddites vernietigden de machines die hun banen wegnamen, en mensen waren bang voor de eerste treinen en auto's.
Deze mensen hebben altijd ongelijk gehad.

### We willen niet denken aan onze dood

De menselijke geest houdt niet van slecht nieuws ontvangen, en heeft verschillende copingmechanismen om ermee om te gaan.
De belangrijkste mechanismen bij het praten over x-risk zijn [ontkenning](https://en.wikipedia.org/wiki/Cognitive_dissonance) en [compartimentalisatie](<https://en.wikipedia.org/wiki/Compartmentalization_(psychology)>).
Als het gaat om onze eigen dood, zijn we bijzonder geneigd tot ontkenning.
Er zijn boeken geschreven over de [Ontkenning van de Dood](https://en.wikipedia.org/wiki/The_Denial_of_Death).

Deze copingmechanismen beschermen ons tegen de pijn van het moeten accepteren dat de wereld niet is zoals we dachten dat deze was.
Echter, ze kunnen ons ook verhinderen om adequaat te reageren op een bedreiging.

Wanneer je merkt dat iemand deze copingmechanismen gebruikt, probeer dan empathisch te zijn.
Ze doen het niet opzettelijk, en ze zijn niet dom.
Het is een natuurlijke reactie op slecht nieuws, en we doen het allemaal tot op zekere hoogte.

<!-- We avert thinking about negative, hopeless situations. No quick option for most people to engage in. No fear-release response. Sustained hopelessness. -->

### Toegeven dat je werk gevaarlijk is, is moeilijk

Voor degenen die aan AI-capaciteiten hebben gewerkt, is het accepteren van de gevaren ervan nog moeilijker.

Neem bijvoorbeeld Yoshua Bengio.
Yoshua Bengio heeft een briljante geest en is een van de pioniers in AI.
AI-veiligheidsexperts waarschuwen al jaren voor de potentiële gevaren van AI, maar het heeft hem nog steeds lang gekost om hun waarschuwingen serieus te nemen.
In een [interview](https://youtu.be/0RknkWgd6Ck?t%25253D949) gaf hij de volgende verklaring:

> "Waarom dacht ik er niet eerder aan? Waarom dacht Geoffrey Hinton er niet eerder aan? [...] Ik geloof dat er een psychologisch effect is dat voor veel mensen nog steeds aan de orde kan zijn. [...] Het is heel moeilijk, in termen van je ego en je goed voelen over wat je doet, om het idee te accepteren dat hetgene waar je decennia aan hebt gewerkt, in feite heel gevaarlijk voor de mensheid kan zijn. [...] Ik denk dat ik er niet te veel over wilde nadenken, en dat is waarschijnlijk ook het geval voor anderen."

Het zou niemand moeten verbazen dat sommige van de meest felle ontkenners van AI-risico's zelf AI-onderzoekers zijn.

### Gemakkelijk af te doen als samenzwering of cult

In het afgelopen jaar werd de meerderheid van de bevolking geïntroduceerd in het concept van existentiële risico's van AI.
Wanneer mensen hierover horen, zullen ze naar een verklaring zoeken.
De juiste verklaring is dat AI in feite gevaarlijk is, maar dit geloven is moeilijk en eng: het zal leiden tot veel cognitieve wrijving.
Dus mensen zullen bijna direct naar een andere manier zoeken om hun waarnemingen uit te leggen.
Er zijn twee alternatieve verklaringen die veel gemakkelijker te geloven zijn:

1. **Het is allemaal een grote samenzwering**. AI-bedrijven hypen AI om meer financiering te krijgen, en mensen die zich met AI-veiligheid bezighouden, maken gewoon deel uit van deze hype-machine. Dit verhaal past bij verschillende waarnemingen: bedrijven liegen vaak, veel mensen in de AI-veiligheid zijn in dienst van AI-bedrijven, en er zijn een aantal miljardairs die AI-veiligheidsonderzoek financieren. Echter, we kunnen ook uitleggen waarom dit samenzweringsverhaal gewoon niet waar is. Veel van de alarmisten zijn wetenschappers die er niets mee te winnen hebben. De bedrijven kunnen op de een of andere manier profiteren, maar tot zeer recent (mei 2023) waren ze bijna volledig stil over AI-risico's. Dit is logisch, aangezien bedrijven meestal niet profiteren van mensen die bang zijn voor hun product of dienst. We hebben deels geprotesteerd buiten Microsoft en OpenAI _omdat_ we wilden dat ze de risico's erkenden.
2. **Het is een cult**. De groep die in AI-veiligheid gelooft, is gewoon een stel gekke religieuze extremisten die geloven in het einde van de wereld. Dit lijkt ook te kloppen, aangezien mensen in de AI-veiligheidsgemeenschap vaak zeer gepassioneerd zijn over het onderwerp en allerlei in-group jargon gebruiken. Echter, het valt uit elkaar wanneer je erop wijst dat mensen die waarschuwen voor AI-risico's geen enkele organisatie zijn. Het is een grote, diverse groep mensen, er is geen enkele leider, er zijn geen rituelen en er is geen dogma.

Wat deze verklaringen zo overtuigend maakt, is niet alleen dat ze gemakkelijk te begrijpen zijn, of dat ze alle waarnemingen perfect verklaren - de belangrijkste reden is dat ze geruststellend zijn.
Geloven dat mensen waarschuwen voor AI _omdat er een echte bedreiging is_, is eng en moeilijk te accepteren.

## Moeilijk te begrijpen

De argumenten voor AI-existentiële risico's zijn vaak zeer technisch, en we zijn zeer geneigd om AI-systemen te antropomorfiseren.

### AI-afstemming is verrassend moeilijk

Mensen kunnen intuïtief het gevoel hebben dat ze het probleem van AI-afstemming kunnen oplossen.
Waarom niet een [stopknop](https://www.youtube.com/watch?v=3TYT1QfdfsM&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40&index=10) toevoegen? Waarom niet [de AI opvoeden als een kind](https://www.youtube.com/watch?v=eaYIU6YXr3w)? Waarom niet [Asimov's drie wetten](https://www.youtube.com/watch?v=7PKx3kS7f4A)?
In tegenstelling tot de meeste soorten technische problemen, hebben mensen een mening over hoe ze AI-afstemming kunnen oplossen en onderschatten ze de moeilijkheid van het probleem.
Het begrijpen van de werkelijke moeilijkheid ervan kost veel tijd en moeite.

### We antropomorfiseren

We zien gezichten in wolken, en we zien menselijke kwaliteiten in AI-systemen.
Miljoenen jaren evolutie hebben ons tot zeer sociale wezens gemaakt, maar deze instincten zijn niet altijd nuttig.
We hebben de neiging om te denken dat AIs menselijke doelen en motivaties hebben, emoties kunnen voelen en een gevoel van moraliteit hebben.
We verwachten dat een zeer intelligente AI ook zeer wijs en vriendelijk is.
Dit is een van de redenen waarom mensen intuïtief denken dat AI-afstemming gemakkelijk is, en waarom de [Orthogonaliteitstheorie](https://www.youtube.com/watch?v=hEUO6pjwFOo) zo tegenintuïtief kan zijn.

### AI-veiligheid gebruikt complexe taal

Het veld van AI-veiligheid bestaat voornamelijk uit een kleine groep (slimme) mensen die hun eigen jargon hebben ontwikkeld.
Het lezen van LessWrong-berichten kan aanvoelen als het lezen van een vreemde taal.
Veel berichten gaan ervan uit dat de lezer al bekend is met wiskundige concepten, verschillende technische concepten en het jargon van het veld.

## Moeilijk om actie te ondernemen

Zelfs als mensen de argumenten begrijpen, is het nog steeds moeilijk om er actie op te ondernemen.
De impact is te groot, we hebben copingmechanismen die de risico's bagatelliseren, en als we de ernst van de situatie voelen, kunnen we ons machteloos voelen.

### Gebrek aan aangeboren angstreactie

Onze hersenen zijn geëvolueerd om te vrezen voor dingen die gevaarlijk zijn.
We vrezen instinctief hoogtes, grote dieren met scherpe tanden, plotselinge harde geluiden en dingen die in een S-vorm bewegen.
Een superintelligente AI raakt geen van onze primaire angsten.
Bovendien hebben we een sterke angst voor sociale afwijzing of het verliezen van sociale status, wat betekent dat mensen vaak bang zijn om zich uit te spreken over AI-risico's.

### Scope-insensitiviteit

> "Een enkele dood is een tragedie; een miljoen doden is een statistiek." - Joseph Stalin

Scope-insensitiviteit is de menselijke neiging om de impact van grote aantallen te onderschatten.
We geven niet 10 keer zoveel om 1000 mensen die sterven als om 100 mensen die sterven.
Existentiële risico's betekenen de dood van alle 8 miljard mensen op aarde (zonder hun nakomelingen mee te tellen).

Zelfs als er een kans van 1% is dat dit gebeurt, is het nog steeds een heel grote zaak.
Rationeel gezien zouden we deze kans van 1% op 8 miljard doden net zo belangrijk moeten beschouwen als de zekere dood van 80 miljoen mensen.

Als iemand het gevoel heeft dat het einde van de wereld niet zo'n groot probleem is (je zou verrast zijn hoe vaak dit gebeurt), kun je proberen de zaken persoonlijker te maken.
De mensheid is niet zomaar een abstract concept, ze zijn je vrienden, je familie en jezelf.
Alle mensen om wie je geeft, zullen sterven.

### Ons gedrag wordt gevormd door onze omgeving en primitieve geest

Onze acties worden voorwaardelijk door wat als normaal, goed en redelijk wordt gezien.
Hoezeer we ook willen handelen in een situatie die daarom vraagt, als die acties ongewoon zijn, vrezen we vaak bewust of onbewust om door de maatschappij uitgesloten te worden voor het doen ervan. En wat normaal is, wordt in onze geest geduwd door omringd te zijn door het in onze nauwe sociale kringen en online feeds.
Mensen die gewoon dingen doen en erover praten die niet gerelateerd zijn aan wat we eigenlijk belangrijk vinden, zullen wat er in onze geest is, overschaduwen en ons motiveren om andere dingen te doen op dagelijkse basis.

Uitsterf risico's verdienen **veel** meer van onze tijd, energie en aandacht. Onze reacties erop zouden meer moeten lijken op leven-of-dood situaties die ons met adrenaline vullen. Maar, vanwege de abstracte aard van de problemen en onze slecht aangepaste geest, eindigen de meeste mensen die erover leren gewoon met het voortzetten van hun dagen alsof ze niets hebben geleerd.

### Copingmechanismen (voorkomen van actie)

Dezelfde copingmechanismen die mensen verhinderen om _te geloven_ in existentiële risico's, voorkomen ook dat ze _er actie op ondernemen_.
Als je in ontkenning bent of compartimentaliseert, voel je niet de behoefte om er iets aan te doen.

### Stress en angst

Terwijl ik dit schrijf, voel ik me gestrest en angstig.
Het is niet alleen omdat ik vrees voor het einde van de wereld, maar ook omdat ik het gevoel heb dat ik er iets aan moet doen.
Er is veel druk om actie te ondernemen, en het kan overweldigend zijn.
Deze stress kan een goede motivator zijn, maar het kan ook verlammend zijn.

### Hopeloosheid en machteloosheid

Wanneer mensen het onderwerp serieus nemen, en de volledige ernst van de situatie doordringt, kunnen ze het gevoel hebben dat alle hoop verloren is.
Het kan aanvoelen als een kankerdiagnose: je gaat eerder dood dan je wilde, en er is niets dat je eraan kunt doen.
Het probleem is te groot om aan te pakken, en jij bent te klein.
De meeste mensen zijn geen AI-veiligheidsexperts of ervaren lobbyisten, dus hoe kunnen ze er in hemelsnaam iets aan doen?

## Maar je kunt helpen!

Er zijn veel [dingen die je kunt doen](/action).
Een brief schrijven, naar een protest gaan, wat geld doneren of lid worden van een gemeenschap is niet zo moeilijk!
En deze acties hebben een echte impact.
Zelfs wanneer we worden geconfronteerd met het einde van de wereld, kan er nog steeds hoop zijn en zeer lonend werk te doen.
[Word lid van PauseAI](/join) en word deel van onze beweging.
