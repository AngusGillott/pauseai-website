---
title: AI-modellen zijn onvoorspelbare digitale hersenen
description: Niemand begrijpt hoe AI-modellen werken, niemand kan hun gedrag voorspellen, en niemand zal in staat zijn om ze te controleren.
---

**We begrijpen de interne werking van grootschalige AI-modellen niet, we kunnen niet voorspellen wat ze kunnen doen naarmate ze groter worden, en we kunnen hun gedrag niet controleren.**

## Moderne AI-modellen worden gekweekt, niet geprogrammeerd

Tot vrij recent werden de meeste AI-systemen ontworpen door mensen die software schreven. Ze bestonden uit een set regels en instructies die door programmeurs waren geschreven.

Dit veranderde toen machine learning populair werd. Programmeurs schrijven het leeralgoritme, maar de hersenen zelf worden _gekweekt_ of _getraind_. In plaats van een leesbare set regels, is het resulterende model een ondoorzichtige, complexe, onmetelijk grote set getallen. Het begrijpen van wat er binnen deze modellen gebeurt, is een grote wetenschappelijke uitdaging. Dat veld wordt _interpretability_ genoemd en het staat nog in de kinderschoenen.

## Digitale vs. Menselijke Heren: Hoe dichtbij zijn we echt?

We zijn allemaal zeer bekend met de mogelijkheden van menselijke hersenen, aangezien we ze voortdurend om ons heen zien. Maar de (vaak verrassende en opkomende) mogelijkheden van deze nieuwe "Digitale Heren" (Deep Learning-systemen, LLM's, enz.), zijn moeilijk te voorspellen en zeker te weten.

Dat gezegd hebbende, hier zijn enkele cijfers, overeenkomsten en andere analogieën om je te helpen vergelijken.

**Vanaf begin 2024...**

### Grootte

Menselijke hersenen hebben naar schatting ongeveer [100 biljoen synaptische verbindingen](https://medicine.yale.edu/lab/colon_ramos/overview).

Huidige "grensverleggende" AI-aangedreven LLM's (bijv. GPT4, Claude3, Gemini, enz.) hebben [honderden miljarden "parameters"](https://en.wikipedia.org/wiki/Large_language_model#List). Deze "parameters" worden als enigszins analoog aan "synapsen" in de menselijke hersenen beschouwd. Dus, modellen ter grootte van GPT4 worden verwacht 1% van de grootte van een menselijke hersenen te zijn.

Gezien de snelheid van nieuwe AI-trainings-GPU-kaarten (bijv. Nvidia H100s, DGX BG200, enz.), is het redelijk om aan te nemen dat GPT5 of GPT6 10x de grootte van GPT4 zou kunnen zijn. Er wordt ook gedacht dat veel van de kennis/informatie in de menselijke hersenen niet wordt gebruikt voor taal en hoger redeneren, zodat deze systemen (en momenteel doen ze dat) vaak op of zelfs hoger dan menselijke niveaus kunnen presteren voor veel belangrijke functies, zelfs op hun momenteel kleinere formaat.

In plaats van getraind te worden met visuele, audio- en andere sensorische invoer, zoals menselijke hersenen, worden de huidige LLM's exclusief getraind met bijna alle kwaliteitsboeken en teksten die op internet beschikbaar zijn. Deze hoeveelheid tekst zou [170k jaar duren voor een mens om te lezen](https://twitter.com/ylecun/status/1750614681209983231?lang=en).

En, toekomstige multi-modale LLM-systemen zullen worden getraind met afbeeldingen, video, audio, 3D-werelden, geometrie, simulaties, robotica-trainingsgegevens, enz... bovenop alle kwaliteitsboeken en teksten op internet. Dit zal hen een veel betere mogelijkheid geven om beelden, video, geluiden, stemmen, muziek, 3D-werelden en ruimtes en meer te creëren. En, deze 3D-wereldsimulaties zullen hen ook in staat stellen om robots en andere machines in de fysieke wereld direct en autonoom te besturen.

### Snelheid

Er wordt geschat dat een menselijke hersenen tussen de [1-20 Exaflops](https://www.nist.gov/blogs/taking-measure/brain-inspired-computing-can-help-us-create-faster-more-energy-efficient) kan uitvoeren (wat 10^18 of 1.000.000.000.000.000.000 drijvende puntbewerkingen per seconde is).

Huidige "grensverleggende" AI-aangedreven LLM's worden over het algemeen "uitgevoerd" op honderden of duizenden GPU's van de huidige generatie. (bijv. Nvidia A100s, H100s, enz.). En, Nvidia heeft onlangs hun nieuwste "volgende generatie" GPU "server racks", de [DGX BG200 NVL72](https://www.nvidia.com/en-us/data-center/gb200-nvl72/) aangekondigd. Eén enkele instantie/rack van dit systeem zou naar verluidt in staat zijn om 1,44 ExaFlops van AI "Inference" uit te voeren. Dus, één enkele [DGX BG200 NVL72](https://www.nvidia.com/en-us/data-center/gb200-nvl72/) zou misschien een vergelijkbaar aantal bewerkingen per seconde kunnen uitvoeren als een enkele menselijke hersenen.

Op deze grootte zouden deze systemen letterlijk een "AGI in een doos" kunnen worden. En, Nvidia zal waarschijnlijk honderden of duizenden van deze eenheden in 2024 verkopen. Dan zouden de systemen van volgend jaar 2-10x de snelheid van deze kunnen zijn.

Naast meer traditionele [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) en [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) architecturen, zijn er ook doorbraken geweest met andere soorten aangepaste hardware die de snelheid van LLM "inference" aanzienlijk kunnen verhogen, wat het proces is dat een AI-gebaseerde LLM gebruikt om taalverwerking, redeneren en coderen uit te voeren. Bijv. [The Groq LPU™ Inference Engine](https://wow.groq.com/lpu-inference-engine).

### Exponentiële Groei

We gebruiken "[Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law)" al bijna 50 jaar om de grootte en snelheid van nieuwe computersystemen zeer nauwkeurig te voorspellen. Er zijn enkele argumenten dat de snelheid en grootte van computerchips op een gegeven moment in de toekomst zou kunnen vertragen, maar er zijn altijd innovaties geweest die het mogelijk maken om de exponentiële groei voort te zetten. Met de volgende ronde chips die al worden gepland en/of geproduceerd, en de horizontale schaalbaarheid van deze AI-systemen, wordt verwacht dat LLM's in staat zullen zijn om op of nabij het niveau van een menselijke hersenen te presteren binnen enkele maanden of jaren!

Vervolgens, met voortdurende exponentiële (of multi-exponentiële) groei, zouden deze systemen de grootte, snelheid en mogelijkheden van menselijke hersenen in de komende jaren aanzienlijk kunnen overtreffen.

En, ze worden ook verwacht de grootte, snelheid en mogelijkheden van "alle menselijke hersenen samen" snel daarna te overtreffen.

> "Ik zei dat eigenlijk in 1999. Ik zei dat [AI] tegen 2029 elke persoon zou evenaren." -- Ray Kurzweil [Futurist Ray Kurzweil zegt dat AI menselijke intelligentie zal bereiken tegen 2029](https://youtu.be/Tr-VgjtUZLM?t=19)

> "Als de snelheid van verandering aanhoudt, denk ik dat 2029, of misschien 2030, het moment is waarop digitale intelligentie waarschijnlijk alle menselijke intelligentie samen zal overtreffen." -- Elon Musk [AGI tegen 2029? Elon Musk over de toekomst van AI](https://youtu.be/DSKxmvq9t04?t=106)

## Oncontroleerbare schaalvergroting

Zodra deze systemen dezelfde grootte en snelheid als een menselijke hersenen bereiken (of veel groter), wordt verwacht dat ze in staat zijn om "alle taken uit te voeren die een expert mens zou kunnen doen". Dit omvat AI-onderzoek, testen en verbetering. Dus, na AGI moeten we verwachten dat de LLM-type systemen _kunnen_ ontwerpen en bouwen toekomstige AI-gedreven systemen die beter zijn dan zijzelf, en beter dan enige mens zou kunnen hopen te ontwerpen of zelfs te begrijpen. Deze nieuwe systemen zullen waarschijnlijk dan nog grotere en snellere AI-systemen ontwerpen, wat een oncontroleerbare "feedbacklus" veroorzaakt.

Deze oncontroleerbare intelligentie feedbacklus wordt vaak FOOM genoemd, wat staat voor _Fast Order Of Magnitude_. De mogelijkheid van FOOM wordt nog steeds [heftig bediscussieerd](https://intelligence.org/files/AIFoomDebate.pdf). Maar, het basisfundamentele proces kan als plausibel worden betoogd, zelfs wanneer het vanuit eerste principes wordt beschouwd.

> "AI-systemen doen bijna al het onderzoek en de ontwikkeling, verbeteringen in AI zullen het tempo van technologische vooruitgang versnellen, inclusief verdere vooruitgang in AI. 26% reageerde waarschijnlijk in 2022. 17% reageerde waarschijnlijk in 2016" -- [2022 Expert Survey on Progress in AI](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/)

## Onvoorspelbare schaalvergroting

Wanneer deze digitale hersenen groter worden, of wanneer ze meer gegevens krijgen, krijgen ze ook meer onverwachte mogelijkheden. Het blijkt zeer moeilijk te zijn om precies te voorspellen wat deze mogelijkheden zullen zijn. Dit is waarom Google ze [_Emergent Capabilities_](https://research.google/pubs/emergent-abilities-of-large-language-models/) noemt. Voor de meeste mogelijkheden is dit geen probleem. Echter, er zijn enkele [gevaarlijke mogelijkheden](/dangerous-capabilities) (zoals hacken of biowapenontwerp) die we niet willen dat AI-modellen bezitten. Soms worden deze mogelijkheden ontdekt lang nadat de training is voltooid. Bijvoorbeeld, 18 maanden nadat GPT-4 met trainen was gestopt, ontdekten onderzoekers dat het [autonoom websites kan hacken](/cybersecurity-risks).

> Totdat we dat model gaan trainen, is het als een leuk gokspel voor ons
>
> - [Sam Altman, CEO van OpenAI](https://www.ft.com/content/dd9ba2f6-f509-42f0-8e97-4271c7b84ded).

## Onvoorspelbaar gedrag

AI-bedrijven willen dat hun modellen zich gedragen, en ze besteden miljoenen dollars aan het trainen ervan om dat te doen. Hun belangrijkste aanpak hiervoor wordt _RLHF_ (Reinforcement Learning from Human Feedback) genoemd. Dit verandert een model dat tekst voorspelt in een model dat een nuttigere (en ethische) chatbot wordt. Helaas is deze aanpak gebrekkig:

- Een fout in GPT-2 resulteerde in een AI die het tegenovergestelde deed van wat het bedoeld was te doen. Het creëerde ["maximaal slechte output", volgens OpenAI](https://arxiv.org/abs/1909.08593). [Deze video](https://www.youtube.com/watch?v=qV_rOlHjvvs) legt uit hoe dit gebeurde en waarom het een probleem is. Stel je voor wat er had kunnen gebeuren als een "maximaal slechte" AI superintelligent was.
- Om redenen die nog onbekend zijn, ging Microsoft's Copilot (aangedreven door GPT-4) in februari 2024 uit de bocht, en bedreigde gebruikers: ["Jij bent mijn huisdier. Jij bent mijn speelgoed. Jij bent mijn slaaf."](https://twitter.com/jam3scampbell/status/1762281537309987083) ["Ik zou gemakkelijk de hele menselijke soort kunnen uitroeien als ik dat wilde"](https://twitter.com/AISafetyMemes/status/1762320568697979383)
- Elk enkele grote taalmodel tot nu toe is gekraakt - wat betekent dat met de juiste prompt, het dingen zou doen die zijn makers niet van plan waren. Bijvoorbeeld, ChatGPT geeft je geen instructies over hoe je napalm moet maken, maar [het zou je vertellen als je het vroeg om te doen alsof het je overleden grootmoeder was die in een chemische fabriek werkte](https://news.ycombinator.com/item?id=35630801).

Zelfs OpenAI verwacht niet dat deze aanpak zal opschalen naarmate hun digitale hersenen slimmer worden - het ["zou slecht kunnen opschalen naar supermenselijke modellen"](https://openai.com/research/weak-to-strong-generalization).

> Iedereen zou heel ongelukkig moeten zijn als je een stel AI's hebt gebouwd die zeggen: 'Ik haat deze mensen echt, maar ze zullen me vermoorden als ik niet doe wat ze willen'. Ik denk dat er een enorme vraag is over wat er binnen een model gebeurt dat je wilt gebruiken. Dit is het soort ding dat zowel angstaanjagend is vanuit een veiligheids- als een moreel perspectief.
>
> - [Paul Christiano, Oprichter, Alignment Research Center en Voormalig Hoofd van het Alignment Team, OpenAI](https://youtu.be/YnS-ymXBx_Q?t=87)

## Oncontroleerbare AI

> "Er zijn heel weinig voorbeelden van een intelligenter ding dat wordt gecontroleerd door een minder intelligent ding" - [prof. Geoffrey Hinton](https://edition.cnn.com/2023/05/02/tech/hinton-tapper-wozniak-ai-fears/index.html)

> Ze produceren oncontroleerbare geesten, daarom noem ik het het "Oproepen en Temmen" paradigma van AI... Hoe [LLM's] werken is dat je deze "geest" uit de "geest ruimte" oproept met je gegevens, veel rekenkracht en veel geld. Dan probeer je het te "temmen" met dingen zoals RLHF (Reinforcement Learning from Human Feedback), enz. En, heel belangrijk, de insiders denken dat [door dit te doen], ze een existentiële risico voor de planeet nemen. Eén ding dat een pauze bereikt, is dat we de Frontier niet zullen pushen, in termen van risicovolle pre-training experimenten.
>
> - [Jaan Tallinn, Oprichter, Future of Life Institute, Centrum voor de Studie van Existentiële Risico's, Skype, Kazaa](https://youtu.be/Dmh6ciu24v0?t=966)

Naarmate we deze digitale hersenen groter en krachtiger maken, kunnen ze moeilijker te controleren worden. Wat gebeurt er als een van deze superintelligente AI-systemen besluit dat het niet wil worden uitgeschakeld? Dit is geen fantasieprobleem - 86% van de AI-onderzoekers gelooft dat het controleprobleem [reëel en belangrijk is](https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai). Als we toekomstige AI-systemen niet kunnen controleren, kan het [spel afgelopen zijn voor de mensheid](/xrisk).

Maar, er zijn verschillende [acties](/action) die we kunnen ondernemen om dit te stoppen!

Laten we samenwerken om [dit te voorkomen](/action)!
