---
title: FAQ
description: Veelgestelde vragen over PauseAI en de risico's van superintelligente AI.
---

<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

## Inhoud

## Wie zijn jullie?

Wij zijn een gemeenschap van [vrijwilligers](/people) en [lokale gemeenschappen](/communities) gecoördineerd door een [non-profit](/legal) die zich richt op het verminderen van de [risico's van AI](/risks) (inclusief het [risico van menselijke uitsterving](/xrisk)).
We willen onze regeringen overtuigen om in te grijpen en [de ontwikkeling van supermenselijke AI te pauzeren](/proposal).
We doen dit door het publiek te informeren, met besluitvormers te praten en protesten te organiseren.

Je kunt ons vinden op [Discord](https://discord.gg/2XXWXvErfA) (dit is waar de meeste coördinatie plaatsvindt!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI), [Instagram](https://www.instagram.com/pause_ai), [Telegram](https://t.me/+UeTsIsNkmt82ZmQ8), [Whatsapp](https://chat.whatsapp.com/JgcAbjqRr8X3tvrXdeQvfj) en [Reddit](https://www.reddit.com/r/PauseAI/).
Je kunt ons mailen/contacten op [joep@pauseai.info](mailto:joep@pauseai.info).

## Zijn jullie niet gewoon bang voor veranderingen en nieuwe technologie?

Je zou verrast kunnen zijn dat de meeste mensen in PauseAI zichzelf als techno-optimisten beschouwen.
Veel van hen zijn betrokken bij AI-ontwikkeling, zijn gadgetliefhebbers en zijn over het algemeen erg enthousiast over de toekomst.
Bijzonder veel van hen zijn enthousiast over het potentieel van AI om de mensheid te helpen.
Daarom was de trieste realisatie dat AI een existentiëel risico kan zijn voor velen een zeer [moeilijke om te internaliseren](/psychology-of-x-risk).

## Willen jullie alle AI verbieden?

Nee, alleen de ontwikkeling van de grootste algemene AI-systemen, vaak "Frontier-modellen" genoemd.
Bijna alle momenteel bestaande AI zou legaal zijn onder [ons voorstel](/proposal), en de meeste toekomstige AI-modellen zullen ook legaal blijven.
We roepen op tot een verbod op AI-systemen die krachtiger zijn dan GPT-4, totdat we weten hoe we aantoonbaar veilige AI kunnen bouwen, en we deze onder democratische controle hebben.

## Geloven jullie dat GPT-4 ons gaat doden?

Nee, we denken niet dat [huidige AI-modellen](/sota) een existentiële bedreiging vormen.
Het lijkt waarschijnlijk dat de meeste volgende AI-modellen dat ook niet zullen zijn.
Maar als we blijven bouwen aan steeds krachtigere AI-systemen, zullen we uiteindelijk een punt bereiken waarop er één een [existentiële bedreiging](/xrisk) wordt.

## Kan een Pauze averechts werken en dingen erger maken?

We hebben deze zorgen behandeld in [dit artikel](/mitigating-pause-failures).

## Is een Pauze überhaupt mogelijk?

AGI is niet onvermijdelijk.
Het vereist hordes ingenieurs met miljoen-dollar salarissen.
Het vereist een volledig functionele en onbeperkte toeleveringsketen van de meest complexe hardware.
Het vereist dat we allemaal toestaan dat deze bedrijven gokken met onze toekomst.

[Lees meer over de haalbaarheid van een Pauze](/feasibility).

## Wie betaalt jullie?

Bijna al onze acties tot nu toe zijn uitgevoerd door vrijwilligers.
Echter, sinds februari 2024 is PauseAI een [geregistreerde non-profit stichting](/legal), en we hebben meerdere donaties van individuen ontvangen.
We hebben ook 20k financiering ontvangen van het LightSpeed-netwerk.

Je kunt ook [doneren](/donate) aan PauseAI als je onze zaak steunt!
We gebruiken het meeste geld om lokale gemeenschappen in staat te stellen evenementen te organiseren.

## Wat zijn jullie plannen?

Focus op [het laten groeien van de beweging](/growth-strategy), protesten organiseren, politici lobbyen en het publiek informeren.

Bekijk onze [roadmap](/roadmap) voor een gedetailleerd overzicht van onze plannen en wat we zouden kunnen doen met meer financiering.

## Hoe denken jullie dat jullie regeringen kunnen overtuigen om AI te pauzeren?

Bekijk onze [theorie van verandering](/theory-of-change) voor een gedetailleerd overzicht van onze strategie.

## Waarom protesteren jullie?

- Protesteren laat de wereld zien dat we om dit probleem geven. Door te protesteren tonen we aan dat we bereid zijn onze tijd en energie te besteden om mensen te laten luisteren.
- Protesten kunnen en zullen vaak [positief invloed hebben](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf) op de publieke opinie, stemgedrag, bedrijfsvoering en beleid.
- Bijna [de meeste mensen zijn ondersteunend](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america) tegenover vreedzame/niet-gewelddadige protesten.
- Er is [geen "terugslag" effect](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) [tenzij het protest gewelddadig is](https://news.stanford.edu/2018/10/12/how-violent-protest-can-backfire/). Onze protesten zijn vreedzaam en niet-gewelddadig.
- Het is een sociale bindingservaring. Je ontmoet andere mensen die jouw zorgen delen en bereid zijn om actie te ondernemen.
- Bekijk [dit geweldige artikel](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) voor meer inzichten over waarom protesteren werkt.

Als je wilt [een protest organiseren](/organizing-a-protest), kunnen we je helpen met advies en middelen.

## Hoe waarschijnlijk is het dat superintelligente AI zeer slechte uitkomsten zal veroorzaken, zoals menselijke uitsterving?

We hebben [een lijst van 'p(doom)' waarden](/pdoom) (kans op slechte uitkomsten) samengesteld van verschillende opmerkelijke experts op dit gebied.

AI-veiligheidsonderzoekers (die de experts op dit onderwerp zijn) zijn verdeeld over deze vraag, en schattingen [variëren van 2% tot 97% met een gemiddelde van 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Let op dat geen (ondervraagde) AI-veiligheidsonderzoekers geloven dat er een kans van 0% is.
Echter, er kan hier sprake zijn van selectie-bias: mensen die in het veld van AI-veiligheid werken, doen dat waarschijnlijk omdat ze geloven dat het voorkomen van slechte AI-uitkomsten belangrijk is.

Als je AI-onderzoekers in het algemeen vraagt (niet veiligheidsspecialisten), dan daalt dit aantal naar een [gemiddelde waarde van ongeveer 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), met een mediaan van 5%.
Een minderheid, ongeveer 20% van hen, gelooft dat het afstemprobleem geen echt of belangrijk probleem is.
Let op dat er hier mogelijk een selectie-bias in de tegenovergestelde richting is: mensen die in AI werken, doen dat waarschijnlijk omdat ze geloven dat AI voordelig zal zijn.

_Stel je voor dat je uitgenodigd wordt voor een testvlucht op een nieuw vliegtuig_.
De vliegtuigingenieurs denken dat er een kans van 14% is dat het crasht.
Zou je dat vliegtuig instappen? Want op dit moment stappen we allemaal aan boord van het AI-vliegtuig.

## Hoe lang hebben we nog tot superintelligente AI?

Het kan maanden duren, het kan decennia duren, niemand weet het zeker.
We weten echter dat het tempo van AI-vooruitgang vaak ernstig wordt onderschat.
Slechts drie jaar geleden dachten we dat we SAT-slaagde AI-systemen in 2055 zouden hebben.
We bereikten dat in april 2023.
We moeten handelen alsof we heel weinig tijd over hebben, omdat we niet verrast willen worden.

[Lees meer over urgentie](/urgency).

## Als we pauzeren, wat dan met China?

Voor starters, op dit moment heeft China strengere AI-regelgeving dan vrijwel elk ander land.
Ze [stonden zelfs geen chatbots toe](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) en [verboden training op internetdata](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) tot [september 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation).
China heeft een meer controlerende regering en heeft dus nog meer reden om bang te zijn voor de oncontroleerbare en onvoorspelbare gevolgen van AI.
Tijdens de UNSC-vergadering over AI-veiligheid was China het enige land dat de mogelijkheid noemde om een pauze in te voeren.

Let ook op dat we voornamelijk vragen om een _internationale_ pauze, afgedwongen door een verdrag.
Zo'n verdrag moet ook door China worden ondertekend.
Als het verdrag garandeert dat andere landen ook zullen stoppen, en er voldoende handhavingsmechanismen zijn,
zou dit iets moeten zijn dat China ook wil zien.

## OpenAI en Google zeggen dat ze gereguleerd willen worden. Waarom protesteren jullie tegen hen?

We prijzen [OpenAI](https://openai.com/blog/governance-of-superintelligence) en [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) voor hun oproepen tot internationale regulering van AI.
Echter, we geloven dat de huidige voorstellen niet genoeg zijn om een AI-catastrofe te voorkomen.
Google en Microsoft hebben nog niet publiekelijk iets gezegd over het existentiële risico van AI.
Alleen OpenAI [noemt expliciet het risico van uitsterving](https://openai.com/blog/governance-of-superintelligence), en opnieuw prijzen we hen voor het serieus nemen van dit risico.
Echter, hun strategie is vrij expliciet: een Pauze is onmogelijk, we moeten eerst naar superintelligentie komen.
Het probleem hiermee is echter dat ze [niet geloven dat ze het afstemprobleem hebben opgelost](https://youtu.be/L_Guz73e6fw?t=1478).
De AI-bedrijven zijn verwikkeld in een race naar de bodem, waarbij AI-veiligheid wordt opgeofferd voor concurrentievoordeel.
Dit is simpelweg het resultaat van marktdynamiek.
We hebben regeringen nodig om in te grijpen en beleid (op internationaal niveau) te implementeren dat [de slechtste uitkomsten voorkomt](/proposal).

## Dringen AI-bedrijven het narratief van existentiële risico's op om ons te manipuleren?

We kunnen niet met zekerheid weten welke motivaties deze bedrijven hebben, maar we weten wel dat **x-risk aanvankelijk niet werd gepromoot door AI-bedrijven - het waren wetenschappers, activisten en NGO's**.
Laten we naar de tijdlijn kijken.

Er zijn veel mensen geweest die sinds het begin van de jaren 2000 hebben gewaarschuwd voor x-risk.
Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark en vele anderen.
Ze hadden geen AI-technologie om te pushen - ze waren simpelweg bezorgd over de toekomst van de mensheid.

De AI-bedrijven hebben x-risk nooit genoemd tot zeer recentelijk.

Sam Altman is een interessante uitzondering.
Hij schreef over existentiële AI-risico's [terug in 2015, op zijn persoonlijke blog](https://blog.samaltman.com/machine-intelligence-part-1), voordat hij OpenAI oprichtte.
In de jaren daarna heeft hij vrijwel geen expliciete vermelding van x-risk meer gemaakt.
Tijdens de hoorzitting in de Senaat op 16 mei 2023, toen hem naar zijn x-risk blogpost werd gevraagd, antwoordde hij alleen door te praten over banen en de economie.
Hij promootte hier het x-risk narratief niet, hij vermeed het actief.

In mei 2023 veranderde alles:

- Op 1 mei [vertrekt 'Godfather of AI' Geoffrey Hinton](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) bij Google om te waarschuwen voor x-risk.
- Op 5 mei werd [de eerste PauseAI-protest aangekondigd](https://twitter.com/Radlib4/status/1654262421794717696), precies voor de deur van OpenAI.
- Op 22 mei publiceerde OpenAI [een blogpost over het bestuur van superintelligentie](https://openai.com/blog/governance-of-superintelligence), en noemde x-risk voor het eerst.
- Op 24 mei erkent ex-CEO van Google Eric Schmidt x-risk.
- Op 30 mei werd de [Safe.ai verklaring (die x-risk erkent)](https://www.safe.ai/statement-on-ai-risk) gepubliceerd. Dit keer, inclusief mensen van OpenAI, Google en Microsoft.

Deze bedrijven hebben zeer traag x-risk erkend, terwijl veel van hun werknemers er al jaren van op de hoogte zijn.
Dus naar onze mening duwen de AI-bedrijven het x-risk narratief niet, ze hebben gereageerd op anderen die het pushen, en hebben gewacht met hun reactie totdat het absoluut noodzakelijk was.

De zakelijke prikkels wijzen in de andere richting: bedrijven zouden liever niet willen dat mensen zich zorgen maken over de risico's van hun producten.
Bijna alle bedrijven bagatelliseren risico's om klanten en investeringen aan te trekken, in plaats van ze te overdrijven.
Hoeveel strikte regulering en negatieve aandacht nodigen de bedrijven uit door deze gevaren toe te geven?
En zou een bedrijf als OpenAI [20% van zijn rekencapaciteit](https://openai.com/blog/introducing-superalignment) aan AI-veiligheid toewijzen als het niet in de risico's gelooft?

Hier is onze interpretatie: de AI-bedrijven hebben de verklaring ondertekend omdat _ze weten dat x-risk een probleem is dat zeer serieus moet worden genomen_.

Een grote reden waarom veel andere mensen nog steeds niet willen geloven dat x-risk een echte zorg is?
Omdat erkennen dat _we in feite in gevaar zijn_ een zeer, zeer beangstigende zaak is.

[Lees meer over de psychologie van x-risk](/psychology-of-x-risk).

## Oké, ik wil helpen! Wat kan ik doen?

Er zijn veel [dingen die je kunt doen](/action).
Op je eentje kun je een [brief schrijven](/writing-a-letter), [flyers posten](/flyering), [leren](/learn) en anderen informeren, deelnemen aan een [protest](/protests), of [doneren](/donate) wat geld!
Maar nog belangrijker: je kunt [lid worden van PauseAI](/join) en coördineren met anderen die actie ondernemen.
Bekijk of er [lokale gemeenschappen](/communities) in jouw omgeving zijn.
Als je meer wilt bijdragen, kun je vrijwilliger worden en je aansluiten bij een van onze [teams](/teams), of [een lokale gemeenschap opzetten](/local-organizing)!

Zelfs wanneer we geconfronteerd worden met het einde van de wereld, kan er nog steeds hoop zijn en zeer lonend werk te doen. 💪
