---
title: Risico's van kunstmatige intelligentie
description: AI bedreigt onze democratie, onze technologie en onze soort.
---

AI is een krachtige technologie die onze wereld steeds meer transformeert.
Het biedt geweldige mogelijkheden, maar ook een enorme hoeveelheid ernstige risico's.
Dit is een poging om alles op te nemen wat kan worden verminderd door een Pauze.

## Huidige gevaren

### Nepnieuws, polarisatie en bedreiging van de democratie

Veel van onze samenleving is gebaseerd op vertrouwen. We vertrouwen erop dat het geld op onze bankrekening echt is, dat het nieuws dat we lezen waar is, en dat de mensen die online beoordelingen plaatsen bestaan.

AI-systemen zijn uitzonderlijk goed in het creëren van nepmedia.
Ze kunnen nepvideo's, nepgeluid, neptekst en nepafbeeldingen maken.
Het creëren van nepmedia is niet nieuw, maar AI maakt het veel goedkoper en veel realistischer.
Deze mogelijkheden verbeteren snel.

Slechts twee jaar geleden lachten we om de vreselijk onrealistische Dall-E afbeeldingen, maar nu hebben we [deepfake-afbeeldingen die fotografiecompetities winnen](https://www.theguardian.com/technology/2023/apr/17/photographer-admits-prize-winning-image-was-ai-generated).
Een door AI gegenereerde afbeelding van een explosie veroorzaakte [paniekverkopen op Wall Street](https://www.euronews.com/next/2023/05/23/fake-news-about-an-explosion-at-the-pentagon-spreads-on-verified-accounts-on-twitter).
Een audiofragment van 10 seconden of een enkele foto kan voldoende zijn om een overtuigende deepfake te creëren.
Misschien nog gevaarlijker dan de deepfakes zelf, is hoe het bestaan van overtuigende deepfakes het vertrouwen vernietigt.
[Een echte afbeelding kan als AI-gegenereerd worden bestempeld](https://www.axios.com/2024/08/13/trump-crowd-photo-ai-deepfake-truth), en mensen zullen het geloven.

GPT-4 kan schrijven op een manier die niet te onderscheiden is van mensen, maar dan veel sneller en voor een fractie van de kosten.
We zouden binnenkort kunnen zien dat sociale media overspoeld worden met nepdiscussies en meningen, en nepnieuwsartikelen die niet te onderscheiden zijn van echte.

Dit leidt tot polarisatie tussen verschillende groepen mensen die in verschillende informatiebronnen en verhalen geloven en, door het consumeren van vervormde representaties van wat er gebeurt, hun verschillen escaleren totdat ze culmineren in gewelddadige en anti-democratische reacties.

Een stop op de grensmodellen (onze [voorstel](/proposal)) zou de modellen die tegenwoordig worden gebruikt om nepmedia te creëren niet stoppen, maar het zou kunnen helpen om toekomstige geavanceerde modellen te voorkomen.
Ook zou het de basis leggen voor toekomstige regelgeving gericht op het verminderen van nepmedia en elk ander specifiek probleem veroorzaakt door AI.
Om nog maar te zwijgen van het vergroten van de publieke aandacht en bewustwording van deze gevaren en het bewijs dat ze kunnen worden aangepakt.

### Deepfakes en impersonatie

Nepinhoud die met AI is gemaakt, ook wel deepfakes genoemd, kan niet alleen de identiteiten van beroemde mensen stelen en [desinformatie creëren](https://time.com/6565446/biden-deepfake-audio/), maar ze kunnen ook jou imiteren.
Iedereen met foto's, video's of audio van iemand en voldoende kennis kan deepfakes van hen maken en deze gebruiken om fraude te plegen, hen te intimideren of seksueel niet-consensueel materiaal te creëren.
Ongeveer [96% van alle deepfake-inhoud is seksueel materiaal](https://www.technologyreview.com/2019/10/07/132735/deepfake-porn-deeptrace-legislation-california-election-disinformation/).

Zoals het gedeelte over nepnieuws zegt, zouden nepmedia niet helemaal worden voorkomen door ons voorstel, maar ze zouden tot op zekere hoogte kunnen worden verminderd.
Een niet zo kleine mate als je in aanmerking neemt dat AI-multipurpose systemen zoals chatbots echt populair zijn geworden, en we hen zouden stoppen om capabeler en populairder te worden, wat systemen zou kunnen omvatten die zijn ontworpen met minder filters en trainbaar zijn met nieuwe gezichten.

### Vooroordelen en discriminatie

AI-systemen worden getraind op data, en veel van de data die we hebben is op de een of andere manier bevooroordeeld.
Dit betekent dat AI-systemen de vooroordelen van onze samenleving zullen overnemen.
Een geautomatiseerd wervingssysteem bij Amazon [nam een vooroordeel tegen vrouwen over](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
Zwarte patiënten werden [minder vaak doorverwezen naar een medisch specialist](https://www.science.org/doi/full/10.1126/science.aax2342).
Bevooroordeelde systemen die in de wetshandhaving worden gebruikt, zoals voorspellende politietechnologie, kunnen leiden tot oneerlijke targeting van specifieke groepen.
Generatieve AI-modellen kopiëren niet alleen de vooroordelen uit hun trainingsdata, [ze versterken ze](https://www.bloomberg.com/graphics/2023-generative-ai-bias/).
Deze vooroordelen verschijnen vaak zonder dat de makers van het AI-systeem zich ervan bewust zijn.

### Banenverlies, economische ongelijkheid en instabiliteit

Tijdens de industriële revolutie verloren veel mensen hun banen aan machines.
Echter, nieuwe (vaak betere) banen werden gecreëerd, en de economie groeide.
Deze keer zou het anders kunnen zijn.

AI vervangt niet alleen onze spieren zoals de stoommachine deed, het vervangt onze hersenen.
Reguliere mensen hebben misschien niets meer te bieden aan de economie.
Afbeeldinggeneratiemodellen (die zwaar zijn getraind op auteursrechtelijk beschermd materiaal van professionele kunstenaars) hebben al [impact op de creatieve industrie](https://cointelegraph.com/news/artists-face-a-choice-with-ai-adapt-or-become-obsolete).
Schrijvers zijn [aan het staken](https://www.newscientist.com/article/2373382-why-use-of-ai-is-a-major-sticking-point-in-the-ongoing-writers-strike/).
GPT-4 heeft [het staatsexamen gehaald](https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/), kan uitstekende geschreven inhoud maken en kan code schrijven (opnieuw, gedeeltelijk getraind op [auteursrechtelijk beschermd materiaal](https://www.ischool.berkeley.edu/news/2023/new-research-prof-david-bamman-reveals-chatgpt-seems-be-trained-copyrighted-books)).

De mensen die deze AI-systemen bezitten, zullen in staat zijn om ervan te profiteren, maar de mensen die hun banen aan hen verliezen, zullen dat niet kunnen.
Het is moeilijk te voorspellen welke banen als eerste zullen worden vervangen.
Ze kunnen je werkloos en zonder inkomen achterlaten, ongeacht hoeveel tijd, geld en energie je hebt besteed aan het verkrijgen van de ervaring en kennis die je hebt, en hoe waardevol ze een moment geleden waren.
De manier waarop we rijkdom in onze samenleving verdelen, is hier niet op voorbereid.

Beleidsmaatregelen zoals een Universeel Basisinkomen zouden de ergste economische gevolgen kunnen voorkomen, maar het is onduidelijk of ze op tijd zullen worden geïmplementeerd.
Zodra onze banen zijn vervangen, kunnen we zonder onderhandelingsmacht komen te zitten om sociale netten te vragen.

En zelfs als we erin slagen om de problemen rond ongelijkheid en instabiliteit goed te navigeren, kunnen we eindigen in een wereld waarin ons gevoel van doel verloren gaat.
Veel kunstenaars voelen dit al, terwijl ze zien dat hun werk door AI wordt vervangen.
Binnenkort zou het ons allemaal kunnen zijn die zich zo voelen.

### Geestelijke gezondheid, verslaving en disconnectie tussen mensen

Sociale mediabedrijven maken al enige tijd gebruik van AI-systemen om hun winst te maximaliseren, terwijl ze profiteren van onze primatengeest, wat onze geestelijke gezondheid schaadt.
AI-chatbots die gebruikers een romantische relatie aanbieden, hebben het afgelopen jaar een enorme groei doorgemaakt, met meer dan 3 miljard zoekresultaten voor 'AI-vriendin' op Google.
Deze AI-relatie-apps zijn [verslavend gebleken](https://onlinelibrary.wiley.com/doi/10.1002/mar.21899), vooral voor "eenzame kwetsbare mensen".

De bedrijven die deze apps beheren, zijn gestimuleerd om ze zo verslavend mogelijk te maken, en hebben een enorme hoeveelheid macht door het gedrag en de meningen van deze modellen vorm te geven.

Een pauze in de grootste modellen zou kunnen voorkomen dat ze multipurpose chatbots worden die perfect aan onze behoeften voldoen zonder dat mensen de langetermijngevolgen ervan begrijpen.

### Geautomatiseerd onderzoek (verlies van privacy)

We laten veel sporen achter op het web.
De verbanden leggen is moeilijk en tijdrovend, maar AI kan dit nu veel goedkoper maken.
Grote taalmodellen kunnen nu autonoom het web doorzoeken en zijn nu goed genoeg om grote hoeveelheden data te analyseren en interessante details te vinden.
Dit kan worden gebruikt om informatie te achterhalen die anders zeer kostbaar zou zijn om te vinden.

- Vind informatie over waar een individu waarschijnlijk op een bepaald moment is. Dit kan worden gebruikt om dissidenten op te sporen of moorden te plannen.
- Koppel anonieme accounts op het web aan echte identiteiten. Dit kan worden gebruikt om te achterhalen wie informatie lekt.

### Milieurisico's

Milieuvervuiling begint significant te worden, en de grootste AI-bedrijven zijn van plan hun energieverbruik aanzienlijk te verhogen. Je kunt hier lezen hoe AI het milieu negatief zal beïnvloeden [/environmental].

### Autonome wapens

Bedrijven verkopen al AI-gestuurde wapens aan overheden.
Lanius bouwt [vliegende zelfmoorddrones](https://www.youtube.com/watch?v=G7yIzY1BxuI) die autonoom vijanden identificeren.
Palantir's [AIP-systeem](https://www.youtube.com/watch?v=XEM5qz__HOU) gebruikt grote taalmodellen om gegevens van het slagveld te analyseren en optimale strategieën te bedenken.

Naties en wapenbedrijven hebben gerealiseerd dat AI een enorme impact zal hebben op het overwinnen van hun vijanden.
We zijn een nieuwe wapenwedloop binnengetreden.
Deze dynamiek beloont versnelling en het nemen van kortere wegen.

Op dit moment hebben we nog steeds mensen in de lus voor deze wapens.
Maar naarmate de mogelijkheden van deze AI-systemen verbeteren, zal er steeds meer druk zijn om de machines de macht te geven om te beslissen.
Wanneer we de controle over wapens aan AI delegeren, kunnen fouten en bugs vreselijke gevolgen hebben.
De snelheid waarmee AI informatie kan verwerken en beslissingen kan nemen, kan conflicten binnen enkele minuten escaleren.
Een [recent artikel](https://arxiv.org/pdf/2401.03408.pdf) concludeert dat "modellen de neiging hebben om dynamiek van een wapenwedloop te ontwikkelen, wat leidt tot grotere conflicten, en in zeldzame gevallen zelfs tot de inzet van nucleaire wapens".

Lees meer op [stopkillerrobots.org](https://www.stopkillerrobots.org/military-and-killer-robots/)

## Gevaren in de nabije toekomst

### Macht accumulatie en tirannie

Krachtige AI-modellen kunnen worden gebruikt om meer macht te krijgen.
Deze positieve feedbacklus kan ertoe leiden dat een paar bedrijven of overheden een ongezonde hoeveelheid macht hebben.
Controle hebben over duizenden intelligente, autonome systemen kan worden gebruikt om meningen te beïnvloeden, markten te manipuleren of zelfs oorlog te voeren.
In handen van een autoritaire regering kan dit worden gebruikt om dissidentie te onderdrukken en de macht te behouden.

### Biologische wapens

AI kan kennis toegankelijker maken, wat ook kennis omvat over hoe biologische wapens te creëren. [Dit artikel](https://arxiv.org/abs/2306.03809) toont aan hoe GPT-4 niet-wetenschappelijke studenten kan helpen om een pandemisch pathogeen te creëren:

> In één uur stelden de chatbots vier potentiële pandemische pathogenen voor, legden uit hoe ze kunnen worden gegenereerd uit synthetisch DNA met behulp van reverse genetics, gaven de namen van DNA-synthesebedrijven die waarschijnlijk geen orders zullen screenen, identificeerden gedetailleerde protocollen en hoe deze te troubleshooten, en raadden aan dat iedereen die niet de vaardigheden heeft om reverse genetics uit te voeren, een kernfaciliteit of contractonderzoeksorganisatie moet inschakelen.

Dit soort kennis is nog nooit zo toegankelijk geweest, en we hebben de waarborgen niet om de potentiële gevolgen aan te pakken.

Bovendien kunnen sommige AI-modellen worden gebruikt om volledig nieuwe gevaarlijke pathogenen te ontwerpen.
Een model genaamd MegaSyn ontwierp [40.000 nieuwe chemische wapens / giftige moleculen in één uur](https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx).
Het revolutionaire AlphaFold-model kan de structuur van eiwitten voorspellen, wat ook een [dual-use technologie](https://unicri.it/sites/default/files/2021-12/21_dual_use.pdf) is.
Het voorspellen van eiwitstructuren kan worden gebruikt om "ziekteveroorzakende mutaties te ontdekken met behulp van het genoom van één individu".
Wetenschappers creëren nu zelfs [volledig autonome chemische laboratoria, waar AI-systemen zelfstandig nieuwe chemicaliën kunnen synthetiseren](https://twitter.com/andrewwhite01/status/1670794000398184451).

Het fundamentele gevaar is dat de kosten voor het ontwerpen en toepassen van biologische wapens door AI met een orde van grootte worden verlaagd.

### Computervirussen en cyberaanvallen

Bijna alles wat we tegenwoordig doen, is op de een of andere manier afhankelijk van computers.
We betalen voor onze boodschappen, plannen onze dagen, nemen contact op met onze dierbaren en zelfs rijden met onze auto's met computers.

Moderne AI-systemen kunnen software analyseren en schrijven.
Ze [kunnen kwetsbaarheden vinden](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411) in software, en [ze zouden kunnen worden gebruikt om deze te exploiteren](https://blog.checkpoint.com/2023/03/15/check-point-research-conducts-initial-security-analysis-of-chatgpt4-highlighting-potential-scenarios-for-accelerated-cybercrime/).
Naarmate de mogelijkheden van AI groeien, zullen ook de mogelijkheden van de exploits die ze kunnen creëren groeien.

Zeer krachtige computervirussen zijn altijd extreem moeilijk te creëren geweest, maar AI zou dat kunnen veranderen.
In plaats van een team van bekwame beveiligingsexperts/hackers in te huren om zero-day exploits te vinden, zou je gewoon een veel goedkopere AI kunnen gebruiken om het voor je te doen. Natuurlijk zou AI ook kunnen helpen met cyberdefensie, en het is onduidelijk aan welke kant het voordeel ligt.

[Lees meer over AI en cybersecurity risico's](/cybersecurity-risks)

### Existentiële Risico

Veel AI-onderzoekers waarschuwen dat AI kan leiden tot het einde van de mensheid.

Zeer intelligente dingen zijn zeer krachtig.
Als we een machine bouwen die veel intelligenter is dan mensen, moeten we er zeker van zijn dat het dezelfde dingen wil als wij.
Echter, dit blijkt zeer moeilijk te zijn.
Dit wordt het _afstemmingsprobleem_ genoemd.
Als we dit niet op tijd oplossen, kunnen we eindigen met superintelligente machines die zich niet om ons welzijn geven.
We zouden een nieuwe soort op de planeet introduceren die ons kan overtreffen en ons kan overconcurreren.

[Lees meer over x-risk](/xrisk)

### Menselijke ontmachtiging

Zelfs als we erin slagen om alleen AI-systemen te creëren die we individueel kunnen controleren, kunnen we onze macht om belangrijke beslissingen te nemen geleidelijk verliezen elke keer dat een systeem wordt geïntegreerd in instellingen of het dagelijks leven.
Die processen zouden uiteindelijk meer input van AI-systemen dan van mensen hebben, en, als we niet snel genoeg kunnen coördineren, of we cruciale kennis over de werking van de systemen missen, kunnen we eindigen zonder controle over onze toekomst.

Het zou een beschaving zijn waarin elk systeem optimaliseert voor verschillende doelstellingen, er geen duidelijke richting is voor waar alles naartoe gaat, en er geen manier is om dit te veranderen.
De technische kennis die nodig is om deze systemen te wijzigen, kan in eerste instantie ontbreken of in de loop van de tijd verloren gaan, naarmate we steeds afhankelijker worden van technologie, en de technologie complexer wordt.

De systemen kunnen hun doelen bereiken, maar die doelen kunnen de waarden die ze zouden moeten belichamen niet volledig omvatten. Dit probleem doet zich tot op zekere hoogte al vandaag voor, maar AIs zouden het aanzienlijk kunnen versterken.

### Digitale sentience

Naarmate AI blijft vorderen, kunnen toekomstige systemen ongelooflijk geavanceerd worden, met neurale structuren en functies die meer lijken op de menselijke hersenen.
Deze verhoogde complexiteit kan leiden tot emergente eigenschappen zoals subjectiviteit en/of bewustzijn, zodat die AIs morele overwegingen verdienen en goed behandeld moeten worden.
Het zou zijn als "digitale mensen".
Het probleem is dat, gezien onze huidige gebrek aan kennis over bewustzijn en de aard van neurale netwerken, we geen manier zullen hebben om te bepalen of sommige AIs enige vorm van ervaring zouden hebben en waar de kwaliteit van die ervaringen van zou afhangen.
Als de AIs blijven worden geproduceerd met alleen hun mogelijkheden in gedachten, door een proces dat we niet volledig begrijpen, zullen mensen ze blijven gebruiken als tools, negerend wat hun verlangens zouden kunnen zijn, en dat ze eigenlijk digitale mensen zouden kunnen enslaven.

### Lijden risico's

Het is niet alleen zo dat waarde-lock-in ons zou kunnen laten falen om de beste soort werelden te bereiken, maar het zou ons ook kunnen laten eindigen in dystopieën die erger zijn dan uitsterving en die zich door de hele ruimte-tijd kunnen uitstrekken.

Mogelijke locked-in dystopieën met veel lijden worden _S-risico's_ genoemd en omvatten werelden waarin sentiente wezens worden geenslaved en gedwongen om vreselijke dingen te doen.
Die wezens kunnen mensen, dieren, digitale mensen of enige andere buitenaardse soort zijn die de AI in het heelal kan vinden. Gezien hoe moeilijk we denken dat het is om afstemming volledig op te lossen, hoe slecht we mensen soms behandelen, hoe slecht we de meeste dieren behandelen, en hoe we huidige AIs behandelen, lijkt een toekomst zoals deze niet zo onwaarschijnlijk als we zouden hopen.

## Wat kunnen we doen?

Voor **alle** de hierboven besproken problemen neemt het risico toe naarmate de mogelijkheden van AI verbeteren.
Dit betekent dat de veiligste optie nu is om **te vertragen**.
We moeten de ontwikkeling van krachtigere AI-systemen pauzeren totdat we hebben uitgezocht hoe we met de risico's moeten omgaan.

Zie [ons voorstel](/proposal) voor meer details.
