---
title: Het pauzeren van AI-ontwikkeling kan verkeerd uitpakken. Hoe de risico's te verminderen?
description: Dit artikel behandelt enkele van de risico's van het pauzeren van AI-ontwikkeling en hoe deze te verminderen.
---

We pleiten voor een pauze in de ontwikkeling van grote, algemene AI-modellen.
Zie ons [voorstel](/proposal) voor meer details.

Deze maatregel is niet zonder risico's.
In dit artikel zullen we enkele van deze risico's bespreken en hoe we ze kunnen verminderen.

## Te vroeg pauzeren

Als een AI-pauze plaatsvindt voordat de risico's groot genoeg zijn, kunnen we de voordelen van AI mislopen.
Uiteindelijk moeten we de risico's afwegen tegen de kosten van pauzeren.

In onze visie is de kans dat AI catastrofale risico's [binnenkort](/urgency) veroorzaakt al groot genoeg om op dit moment een pauze te rechtvaardigen.
Zoals Stuart Russell heeft gesteld, moet men, wanneer men geconfronteerd wordt met een onzekere deadline, de actie ondernemen die optimaal zou zijn gezien de kortste tijdsbeperking.

Hoe langer we wachten, hoe meer mensen zullen denken dat een pauze niet mogelijk is, en hoe meer sommige mensen zullen fantaseren en investeren in theoretisch mogelijke AI-toepassingen.
Dus hoe meer geld er zal gaan naar lobbyen tegen mensen zoals wij.

Bovendien kan het van protesteren en lobbyen tot het overtuigen van mensen in de macht, om een verdrag te laten ingaan, vele jaren duren. Niet te vergeten dat, zelfs als dat niet het geval was, vroeg pauzeren ons ademruimte geeft zodat slechte actoren en algoritmische doorbraken ons niet in de afgrond doen vallen.

## Te kort pauzeren / alleen voor 6 maanden

De pauze die we voorstellen is van onbepaalde lengte. We zouden niet moeten hervatten totdat er een goede consensus is dat we weten hoe we afgestemde AIs kunnen ontwikkelen, ongeacht hoe krachtig ze zijn, en dat we de systemen hebben om dit zorgvuldig en democratisch te doen.
Het is NIET zoals de [zes maanden pauze gevraagd door de open brief gepubliceerd door het Future of Life Institute](https://futureoflife.org/open-letter/pause-giant-ai-experiments/).

## Te lang pauzeren

Nick Bostrom, een van de eerste AI-alarmisten, maakt zich zorgen dat [we op een gegeven moment _te veel_ over AI-risico's kunnen piekeren](https://twitter.com/jachaseyoung/status/1723325057056010680), hoewel dat moment nog niet is aangebroken.
Als de bezorgdheid over AI blijft toenemen, en we krijgen een pauze, kan dit leiden tot een situatie waarin de gehele AI-ontwikkeling taboe of illegaal wordt.
Als dat gebeurt, zullen we nooit profiteren van de voordelen van AI, en ondertussen kunnen we andere existentiële risico's tegenkomen die we met de hulp van AI hadden kunnen vermijden.

We kunnen dit risico aanpakken door duidelijk te stellen onder welke voorwaarden de AI-ontwikkeling moet worden hervat.
Zoals we zeiden, stellen we voor dat de AI-ontwikkeling moet worden hervat wanneer het mogelijk wordt om aantoonbaar veilige AI te bouwen.
Bovendien stellen we alleen voor om de ontwikkeling van zeer specifieke soorten modellen te verbieden: de grootste, algemene modellen.
Ondertussen zijn er andere manieren waarop we meer intelligentie kunnen bereiken: meer transparante AI-paradigma's, hersen-computerinterfaces, volledige hersenemulaties, neurale verbeteringen, groei in collectieve intelligentie, genetische bewerking en selectie, en misschien meer.
Die paden naar een grotere intelligentie zouden ons de voordelen kunnen bieden die AGI belooft zonder zoveel van de risico's.

## Centralisatie van AI kan de risico's van overname verergeren

We stellen geen centralisatie van AI-ontwikkeling in één enkele organisatie voor. Dat zou de AI-ontwikkeling beter controleerbaar maken, maar het zou ook een enkel punt van falen creëren, waar menselijke hebzucht en domheid van kunnen profiteren.
Beslissen of een CERN/Apollo/Manhattan-achtig project goed of niet zou zijn, moet multilateraal worden besproken, zodra we al hebben samengewerkt aan een pauze en buiten een race zijn.

## Decentralisatie zal ervoor zorgen dat minder veiligheidsbewuste actoren de race leiden

Als je in de geschiedenis van OpenAI, DeepMind en Anthropic duikt, zul je ontdekken dat al deze zijn opgericht door mensen die behoorlijk bezorgd zijn over AI-risico's.
Op een bepaalde manier hebben we geluk dat de grootste AI-bedrijven op dit moment AI-veiligheid als onderdeel van hun cultuur hebben.
Misschien geeft een pauze een groot aantal bedrijven de tijd om in te halen, wat kan leiden tot een grote groep bedrijven die minder veiligheidsbewust zijn.

Als we om een tijdgebonden pauze vroegen, zou dit een eerlijke zorg zijn.
Maar wat we vragen is een pauze _totdat we kunnen bewijzen dat AI veilig kan worden gebouwd_, zodat we niet eindigen met organisaties die onveilige AI bouwen nadat de pauze is opgeheven.

## Nationale/lokale pauzes kunnen falen

Als één land de AI-ontwikkeling pauzeert, zullen andere landen doorgaan met de ontwikkeling van AI.
We kunnen eindigen in een wereld waar de eerste AGI wordt ontwikkeld door een niet-coöperatieve actor, wat waarschijnlijk een slecht resultaat zal zijn.
De prikkels om individueel te pauzeren zijn zwak, omdat de voordelen van AI-ontwikkeling groot zijn en de risico's van AI-ontwikkeling wereldwijd zijn.
Dit is een klassieke [prisoner's dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) situatie.

De oplossing hiervoor is om de pauze internationaal te maken via een verdrag, wat is wat we voorstellen.
Dit vereist ook een sterk handhavingsmechanisme.
Landen die zich niet aan het verdrag houden, moeten worden bestraft.
Economische sancties kunnen voldoende zijn, maar militaire interventie kan in extreme gevallen noodzakelijk zijn.

Een actor in het bijzonder waarvan sommige mensen geloven dat deze niet zal pauzeren, is China.
We zijn het niet eens met deze beoordeling en je kunt er [hier](/faq#if-we-pause-what-about-china) meer over lezen.

## AI-ontwikkeling kan ondergronds gaan

Als AI-ontwikkeling (boven een bepaalde drempel) wordt verboden, kan het ondergronds gaan.
De potentiële voordelen zijn zo groot dat een rogue (staats)actor zou kunnen besluiten om AI in het geheim te ontwikkelen.
Dat betekent dat de eerste die superintelligentie bereikt een niet-coöperatieve actor zou zijn, wat waarschijnlijk een slecht resultaat zal zijn.

Door GPU-verkopen te volgen, kunnen we grootschalige AI-ontwikkeling detecteren.
Aangezien frontier model GPU-clusters enorme hoeveelheden energie en aangepaste gebouwen vereisen, is de fysieke infrastructuur die nodig is om een groot model te trainen moeilijk te verbergen.

Westerse machten (VS, Nederland en Taiwan) controleren de GPU-leveringsketen sterk genoeg om te voorkomen dat niet-coöperatieve staten GPU's verkrijgen.
Niet-statelijke actoren zullen waarschijnlijk niet in staat zijn om voldoende middelen in het geheim te verzamelen om een AGI te trainen voor minstens een decennium nadat AGI mogelijk wordt door Big Tech-bedrijven.
Bovendien zou het feit dat er niet langer een _zakelijke prikkel_ is, helpen om de hoeveelheid ondergrondse AI-ontwikkeling te verminderen.

## Hardware-overhang kan een snelle opkomst veroorzaken

> Als we hardware R&D niet in de pauze opnemen, zal de prijs-prestatieverhouding van GPU's elke 2,5 jaar blijven verdubbelen, zoals het deed tussen 2006 en 2021.
> Dit betekent dat AI-systemen na tien jaar minstens 16x sneller zullen worden en na twintig jaar 256x sneller, simpelweg door betere hardware.
> Als de pauze in één keer wordt opgeheven, zouden deze hardwareverbeteringen onmiddellijk beschikbaar worden voor het goedkoper trainen van krachtigere modellen—een hardware-overhang.
> Dit zou een snelle en vrij discontinuïteit in AI-capaciteiten veroorzaken, wat potentieel zou leiden tot een snel opkomstscenario en alle risico's die daarmee gepaard gaan.

[_Door Nora Belrose_](https://bounded-regret.ghost.io/ai-pause-will-likely-backfire-by-nora/)

Dit is een serieuze zorg, hoewel er sterke argumenten zijn te maken dat [overhang onwaarschijnlijk is](https://blog.aiimpacts.org/p/are-there-examples-of-overhang-for).

PauseAI steunt ook een pauze op relevante rekencapaciteitsverbeteringen.
Ook, zoals we zeiden, mag de 'play'-knop niet worden ingedrukt als we nog steeds niet weten hoe we veilige AI kunnen bouwen.
En dat omvat de training en implementatie van modellen met geavanceerdere hardware.

## AI-ontwikkeling is noodzakelijk om te leren hoe we AIs veilig kunnen maken

De meeste mensen geloven dat een bepaald niveau van prozaïsche/incrementale afstemming noodzakelijk is, dus als er een volledige pauze zonder uitzonderingen wordt geïmplementeerd, zou er niet genoeg vooruitgang op het gebied van afstemming worden geboekt en uiteindelijk zouden actoren die zich niet om veiligheid en de pauze geven een onafgestemde krachtige AI ontwikkelen.

Dat is een van de redenen waarom we voorstellen om een manier te hebben om bepaalde trainingsruns goed te keuren. Dat zou ons in staat stellen om van grotere systemen te leren als we hun veiligheid kunnen waarborgen.
Echter, in het ergste geval waarin we hun veiligheid niet kunnen waarborgen en niet genoeg vooruitgang boeken in afstemming, hebben we nog steeds de optie om onze intelligentie te vergroten via andere technologieën.

## Algoritmische of runtime-verbeteringen kunnen kleinere modellen ook gevaarlijk maken

We hebben gezien dat veranderingen in trainingsdata, trainingsalgoritmen of runtime-gebruik kunnen leiden tot grote verbeteringen in modelprestaties.
Daarom richten we ons niet alleen op modelgrootte.
We zijn [voorstellen](/proposal) om de ontwikkeling van grote, algemene AI-modellen die ofwel 1) groter zijn dan 10^12 parameters, 2) meer dan 10^25 FLOPs hebben gebruikt voor training of 3) capaciteiten hebben die naar verwachting GPT-4 zullen overschrijden, te pauzeren.
Deze derde voorwaarde is toegevoegd om ook kleinere modellen op te nemen die gevaarlijk kunnen zijn.
Het handhaven van een limiet op capaciteiten is lastig, omdat het moeilijk is om de capaciteiten van een model te voorspellen voordat het is getraind.

Aangezien de inzet zo hoog is, moeten we voorzichtig zijn, dus ondersteunen we ook een pauze op relevante algoritmische en runtime-verbeteringen.
Echter, het handhaven hiervan zal moeilijker zijn dan het handhaven van rekenvoorschriften, omdat hardware gemakkelijker te traceren is dan software.

## Als we alleen algemene AI-modellen verbieden, kunnen we nog steeds AGI krijgen via smalle modellen

We willen gevaarlijke modellen die [gevaarlijke capaciteiten](/dangerous-capabilities) hebben, zoals het manipuleren van mensen, strategisch plannen en code schrijven, beperken.
We willen geen zeer smalle AI-modellen beperken, zoals afbeeldingsclassificeerders die worden gebruikt in zelfrijdende auto's of medische diagnoses.
Gelukkig vallen vrijwel al deze smalle modellen buiten onze [voorgestelde](/proposal) beperkingen, omdat deze modellen relatief klein zijn.

Een voldoende krachtig smal model (getraind op gegevens uit de echte wereld) kan waarschijnlijk generaliseren naar gevaarlijke capaciteiten.
Bijvoorbeeld, een zeer krachtig afbeeldingsgenerator model kan in staat zijn om afbeeldingen van functionele code te maken, of een zeer krachtig videomodel kan in staat zijn om een film te genereren over een AI die een succesvolle overname plant.
Smalle modellen worden vaak beter in hun smalle taak door te generaliseren.
Tot op zekere hoogte is dit wat LLM's zoals ChatGPT zo succesvol maakt: ze zijn alleen getraind om "het volgende woord te voorspellen", maar om hier echt goed in te zijn, moeten ze veel leren over de wereld.

Daarom hebben we "smal" of "algemeen" AI in ons voorstel niet gedefinieerd, maar in plaats daarvan gebruiken we drie voorwaarden met betrekking tot modelgrootte, gebruikte rekenkracht en capaciteiten.

## Als een pauze wordt geïmplementeerd, moeten we een politieke compromis verwachten

We hebben een [specifiek voorstel](/proposal) dat we denken dat optimaal is.
Echter, we moeten niet verwachten dat ons voorstel precies zo wordt geïmplementeerd als we willen.
Politiek is rommelig en onvoorspelbaar, dus we moeten verwachten dat onze lobby-inspanningen vaag richtinggevende effecten hebben, in plaats van precieze effecten.
Als we een of andere vorm van een pauze krijgen, maar het is niet precies wat we willen, kan dit uiteindelijk erger zijn dan helemaal geen pauze.
Bijvoorbeeld:

- Een nationale pauze die het mogelijk maakt dat potentieel slechtere actoren als eerste AGI bereiken
- Een internationale pauze die niet goed wordt gehandhaafd, wat leidt tot een vergelijkbaar resultaat

We kunnen dit verminderen door consistent en duidelijk te zijn in onze communicatie over wat we willen.

## Te laat pauzeren

Dit is het meest voor de hand liggende en meest waarschijnlijke falingsrisico: als we te laat pauzeren, zullen we waarschijnlijk catastrofale risico's tegenkomen.
En dat kan snel gebeuren, zoals we uitleggen op onze [urgentie](/urgency) pagina.

Dit is waarom we uw hulp nodig hebben om nu te pleiten voor een [pauze](/action).
