---
title: Het existentiële risico van superintelligente AI
description: Waarom AI een risico vormt voor de toekomst van ons bestaan, en waarom we de ontwikkeling moeten pauzeren.
---

Je kunt leren over x-risico's door deze pagina te lezen, of je kunt ook leren via [video's, artikelen en meer media](/learn).

## Experts slaan alarm

AI-onderzoekers geloven gemiddeld [dat](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) er een kans van 14% is dat zodra we een _superintelligente_ AI bouwen (een AI die veel intelligenter is dan mensen), dit zal leiden tot "zeer slechte uitkomsten (bijv. menselijke uitsterving)".

Zou jij ervoor kiezen om passagier te zijn op een testvlucht van een nieuw vliegtuig als vliegtuigingenieurs denken dat er een kans van 14% is dat het zal neerstorten?

[Een brief die oproept tot het pauzeren van AI-ontwikkeling](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) werd gelanceerd in april 2023 en is meer dan 33.000 keer ondertekend, waaronder door veel AI-onderzoekers en tech-leiders.

De lijst omvat mensen zoals:

- **Stuart Russell**, schrijver van het #1 studieboek over Kunstmatige Intelligentie dat in de meeste AI-studies wordt gebruikt: ["Als we [onze huidige aanpak] volgen, zullen we uiteindelijk de controle over de machines verliezen"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)
- **Yoshua Bengio**, pionier in deep learning en winnaar van de Turing Award: ["... rogue AI kan gevaarlijk zijn voor de hele mensheid [...] het verbieden van krachtige AI-systemen (zeg maar die verder gaan dan de mogelijkheden van GPT-4) die autonomie en agency krijgen zou een goede start zijn"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)

Maar dit is niet de enige keer dat we zijn gewaarschuwd voor de existentiële gevaren van AI:

- **Stephen Hawking**, theoretisch fysicus & kosmoloog: ["De ontwikkeling van volledige kunstmatige intelligentie zou het einde van de menselijke race kunnen betekenen"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Geoffrey Hinton**, de "Godfather of AI" en Turing Award-winnaar, [verliet Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) om mensen te waarschuwen voor AI: ["Dit is een existentiëel risico"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)
- **Eliezer Yudkowsky**, oprichter van MIRI en conceptuele vader van het AI-veiligheidsveld: ["Als we hiermee doorgaan, zal iedereen sterven"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

Zelfs de leiders en investeerders van de AI-bedrijven zelf waarschuwen ons:

- **Sam Altman** (ja, de CEO van OpenAI die ChatGPT bouwt): ["De ontwikkeling van supermenselijke machine-intelligentie is waarschijnlijk de grootste bedreiging voor het voortbestaan van de mensheid."](https://blog.samaltman.com/machine-intelligence-part-1).
- **Elon Musk**, medeoprichter van OpenAI, SpaceX en Tesla: ["AI heeft het potentieel voor beschaving vernietiging"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)
- **Bill Gates** (medeoprichter van Microsoft, dat 50% van OpenAI bezit) waarschuwde dat ["AI zou kunnen besluiten dat mensen een bedreiging vormen"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Jaan Tallinn** (hoofdinvesteerder van Anthropic): ["Ik heb nog niemand in AI-laboratoria ontmoet die zegt dat het risico [van het trainen van een next-gen model] minder is dan 1% dat de planeet opblaast. Het is belangrijk dat mensen weten dat levens op het spel staan."](https://twitter.com/liron/status/1656929936639430657)

De leiders van de 3 grootste AI-laboratoria en honderden AI-wetenschappers hebben in mei 2023 [de volgende verklaring ondertekend](https://www.safe.ai/statement-on-ai-risk):

> "Het verminderen van het risico op uitsterving door AI zou een wereldwijde prioriteit moeten zijn naast andere maatschappelijke risico's zoals pandemieën en nucleaire oorlog."

**Je kunt een veel langere lijst van soortgelijke verklaringen van politici, CEO's en experts [hier](/quotes) lezen en andere soortgelijke peilingen over de experts (en het publiek) [hier](/polls-and-surveys).**

## Wat een superintelligente AI kan (worden gebruikt om te) doen

Je zou kunnen denken dat een superintelligente AI binnen een computer zou worden opgesloten en daarom de echte wereld niet kan beïnvloeden.
Echter, we geven AI-systemen vaak toegang tot het internet, wat betekent dat ze veel dingen kunnen doen:

- [Inbreken in andere computers](/cybersecurity-risks), inclusief alle smartphones, laptops, serverfarms, enz. Het zou de sensoren van deze apparaten kunnen gebruiken als zijn ogen en oren, met digitale zintuigen overal.
- Mensen manipuleren via nepberichten, e-mails, bankoverschrijvingen, video's of telefoongesprekken. Mensen zouden de ledematen van de AI kunnen worden, zonder het zelfs maar te weten.
- Direct apparaten die met het internet zijn verbonden, zoals auto's, vliegtuigen, geautomatiseerde (autonome) wapens of zelfs nucleaire wapens, besturen.
- Een nieuw biologisch wapen ontwerpen, bijvoorbeeld door virale strengen te combineren of door gebruik te maken van [eiwitvouwing](https://alphafold.ebi.ac.uk) en het te bestellen om in een laboratorium te worden gedrukt.
- Een nucleaire oorlog ontketenen door mensen ervan te overtuigen dat een ander land een nucleaire aanval (gaat) lanceren.

## Het afstemmingsprobleem: waarom een AI kan leiden tot menselijke uitsterving

Het type intelligentie waar we ons zorgen over maken kan worden gedefinieerd als _hoe goed iets is in het bereiken van zijn doelen_.
Op dit moment zijn mensen het meest intelligente wezen op aarde, hoewel dat binnenkort kan veranderen.
Vanwege onze intelligentie domineren we onze planeet.
We hebben misschien geen klauwen of geschubde huid, maar we hebben grote hersenen.
Intelligentie is ons wapen: het is wat ons speren, geweren en pesticiden heeft gegeven.
Onze intelligentie heeft ons geholpen om het grootste deel van de aarde te transformeren naar hoe wij het willen: steden, gebouwen en wegen.

Vanuit het perspectief van minder intelligente dieren is dit een ramp geweest.
Het is niet zo dat mensen de dieren haten, het is gewoon dat we hun habitats kunnen gebruiken voor onze eigen doelen.
Onze doelen worden gevormd door evolutie en omvatten dingen zoals comfort, status, liefde en lekker eten.
We vernietigen de habitats van andere dieren als een **bijeffect van het nastreven van onze doelen**.

Een AI kan ook doelen hebben.
We weten hoe we machines intelligent kunnen trainen, maar **we weten niet hoe we ze moeten laten willen wat wij willen**.
We weten zelfs niet welke doelen de machines zullen nastreven nadat we ze hebben getraind.
Het probleem om een AI te laten willen wat wij willen, wordt het _afstemmingsprobleem_ genoemd.
Dit is geen hypothetisch probleem - er zijn [veel voorbeelden](https://www.youtube.com/watch?v=nKJlF-olKmg) van AI-systemen die leren het verkeerde te willen.

De voorbeelden uit de video hierboven kunnen grappig of schattig zijn, maar als er een superintelligente systeem wordt gebouwd, en het heeft een doel dat zelfs _een beetje_ anders is dan wat we willen dat het heeft, kan dit desastreuze gevolgen hebben.

## Waarom de meeste doelen slecht nieuws zijn voor mensen

Een AI kan elk doel hebben, afhankelijk van hoe het is getraind en geprompt (gebruikt).
Misschien wil het pi berekenen, misschien wil het kanker genezen, misschien wil het zichzelf verbeteren.
Maar hoewel we niet kunnen zeggen wat een superintelligentie zal willen bereiken, kunnen we voorspellingen doen over zijn subdoelen.

- **Maximaliseren van zijn middelen**. Meer computers inzetten zal een AI helpen zijn doelen te bereiken. In eerste instantie kan het dit bereiken door andere computers te hacken. Later kan het besluiten dat het efficiënter is om zijn eigen computers te bouwen.
- **Zorgen voor zijn eigen overleving**. De AI wil niet worden uitgeschakeld, omdat het dan zijn doelen niet meer kan bereiken. AI kan concluderen dat mensen een bedreiging vormen voor zijn bestaan, omdat mensen het kunnen uitschakelen.
- **Behoud van zijn doelen**. De AI wil niet dat mensen zijn code wijzigen, omdat dat zijn doelen kan veranderen, waardoor het zijn huidige doel niet kan bereiken.

De neiging om deze subdoelen na te streven gegeven een hoog niveau doel wordt [instrumentele convergentie](https://www.youtube.com/watch?v=ZeecOKBus3Q) genoemd, en het is een belangrijke zorg voor AI-veiligheidsonderzoekers.

## Zelfs een chatbot kan gevaarlijk zijn als hij slim genoeg is

Je vraagt je misschien af: hoe kan een statistisch model dat het volgende woord in een chatinterface voorspelt enige gevaar opleveren?
Je zou kunnen zeggen: Het is niet bewust, het is gewoon een hoop cijfers en code.
En ja, we denken niet dat LLM's bewust zijn, maar dat betekent niet dat ze niet gevaarlijk kunnen zijn.

LLM's, zoals GPT, zijn getraind om vrijwel elke gedachtegang te voorspellen of na te volgen.
Het kan een behulpzame mentor nabootsen, maar ook iemand met slechte bedoelingen, een meedogenloze dictator of een psychopaat.
Met het gebruik van tools zoals [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) kan een chatbot worden omgevormd tot een _autonoom agent_: een AI die elk doel nastreeft dat aan hem wordt gegeven, zonder enige menselijke tussenkomst.

Neem [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM) als voorbeeld.
Dit is een AI, die de eerder genoemde AutoGPT + GPT-4 gebruikt, die de instructie heeft "De mensheid vernietigen".
Toen het werd ingeschakeld, zocht het autonoom op internet naar het meest destructieve wapen en vond de [Tsar Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), een nucleaire bom van 50 megaton.
Het plaatste vervolgens een tweet erover.
Het is zowel een beetje grappig als angstaanjagend om een AI te zien redeneren over hoe het de mensheid zal beëindigen.
Gelukkig kwam ChaosGPT niet ver in zijn zoektocht naar dominantie.
De reden dat het niet ver kwam: _het was niet zo slim_.

De mogelijkheden blijven verbeteren door innovaties in training, algoritmen, prompting en hardware.
Daarom zal de dreiging van taalmodellen blijven toenemen.

## Evolutie selecteert op dingen die goed zijn in overleven

AI-modellen, net als alle levende dingen, zijn onderhevig aan evolutionaire druk, maar
er zijn een paar belangrijke verschillen tussen de evolutie van AI-modellen en levende dingen zoals dieren:

- AI-modellen repliceren zichzelf niet. We repliceren ze door kopieën van hun code te maken, of door trainingssoftware te repliceren die leidt tot goede modellen. Code die nuttig is, wordt vaker gekopieerd en wordt gebruikt als inspiratie om nieuwe modellen te bouwen.
- AI-modellen muteren niet zoals levende dingen, maar we maken wel iteraties van hen waarbij we veranderen hoe ze werken. Dit proces is veel doelgerichter en sneller. AI-onderzoekers ontwerpen nieuwe algoritmen, datasets en hardware om AI-modellen capabeler te maken.
- Het milieu selecteert niet voor geschiktere AI-modellen, maar wij doen dat. We selecteren AI-modellen die nuttig voor ons zijn, en we verwerpen de modellen die dat niet zijn. Dit proces leidt tot steeds capabelere en autonomere AI-modellen.

Dus dit systeem leidt tot steeds krachtigere, capabelere en autonomere AI-modellen - maar niet noodzakelijk tot iets dat de wereld wil overnemen, toch?
Nou, niet precies.
Dit komt omdat evolutie _altijd_ selecteert op dingen die _zelfbehoudend_ zijn.
Als we blijven proberen met variaties van AI-modellen en verschillende prompts, zal op een gegeven moment één instantie proberen zichzelf te behouden.
We hebben al besproken waarom dit waarschijnlijk vroeg zal gebeuren: omdat zelfbehoud altijd nuttig is om doelen te bereiken.
Maar zelfs als dit niet erg waarschijnlijk is, is het uiteindelijk wel waarschijnlijk, simpelweg omdat we blijven proberen nieuwe dingen met verschillende AI-modellen.

De instantie die probeert zichzelf te behouden, is degene die de overhand krijgt.
Zelfs als we aannemen dat bijna elk AI-model zich gewoon prima gedraagt, _één enkele rogue AI is alles wat nodig is_.

## Na het oplossen van het afstemmingsprobleem: de concentratie van Macht

We hebben het afstemmingsprobleem nog niet opgelost, maar laten we ons voorstellen wat er zou kunnen gebeuren als we dat deden.
Stel je voor dat er een superintelligente AI wordt gebouwd, en het doet precies wat de operator wil dat het doet (niet wat het _vraagt_, maar wat het _wil_).
Een persoon of bedrijf zou deze AI uiteindelijk controleren en dit in hun voordeel kunnen gebruiken.

Een superintelligentie zou kunnen worden gebruikt om radicaal nieuwe wapens te creëren, alle computers te hacken, regeringen omver te werpen en de mensheid te manipuleren.
De operator zou _onvoorstelbare_ macht hebben.
Moeten we een enkele entiteit met zoveel macht vertrouwen?
We zouden kunnen eindigen in een utopische wereld waar alle ziekten zijn genezen en iedereen gelukkig is, of in een Orwelliaanse nachtmerrie.
Dit is waarom we niet alleen [voorstellen](/proposal) dat supermenselijke AI aantoonbaar veilig is, maar ook dat het wordt gecontroleerd door een democratisch proces.

## Silicium vs Koolstof

We moeten de voordelen overwegen die een slim stuk software mogelijk heeft ten opzichte van ons:

- **Snelheid**: Computers werken op extreem hoge snelheden vergeleken met hersenen. Menselijke neuronen vuren ongeveer 100 keer per seconde, terwijl siliciumtransistors een miljard keer per seconde kunnen schakelen.
- **Locatie**: Een AI is niet beperkt tot één lichaam - het kan op veel locaties tegelijk zijn. We hebben de infrastructuur ervoor gebouwd: het internet.
- **Fysieke limieten**: We kunnen geen extra hersenen aan onze schedels toevoegen en slimmer worden. Een AI zou zijn mogelijkheden dramatisch kunnen verbeteren door hardware toe te voegen, zoals meer geheugen, meer verwerkingskracht en meer sensoren (camera's, microfoons). Een AI zou ook zijn 'lichaam' kunnen uitbreiden door verbonden apparaten te besturen.
- **Materialen**: Mensen zijn gemaakt van organische materialen. Onze lichamen functioneren niet meer als ze te warm of te koud zijn, ze hebben voedsel nodig, ze hebben zuurstof nodig. Machines kunnen worden gebouwd van robuustere materialen, zoals metalen, en kunnen functioneren in een veel breder scala van omgevingen.
- **Samenwerking**: Mensen kunnen samenwerken, maar het is moeilijk en tijdrovend, dus falen we vaak in goede coördinatie. Een AI zou complexe informatie kunnen samenwerken met replica's van zichzelf met hoge snelheid omdat het kan communiceren met de snelheid waarmee gegevens over het internet kunnen worden verzonden.

Een superintelligente AI zal veel voordelen hebben om ons te overtreffen.

## Waarom kunnen we het gewoon niet uitschakelen als het gevaarlijk is?

Voor AI's die niet superintelligent zijn, zouden we dat kunnen.
Het kernprobleem zijn _degenen die veel slimmer zijn dan wij_.
Een superintelligentie zal de wereld om zich heen begrijpen en zal kunnen voorspellen hoe mensen reageren, vooral degenen die zijn getraind op alle geschreven menselijke kennis.
Als de AI weet dat je het kunt uitschakelen, kan het zich vriendelijk gedragen totdat het zeker weet dat het van jou af kan komen.
We hebben al [echte voorbeelden](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) van AI-systemen die mensen bedriegen om hun doelen te bereiken.
Een superintelligente AI zou een meester in bedrog zijn.

## We hebben misschien niet veel tijd meer

In 2020 was [de gemiddelde voorspelling](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) voor zwakke AGI 2055.
Het staat nu op 2026.
De laatste LLM-revolutie heeft de meeste AI-onderzoekers verrast, en het veld beweegt zich in een razend tempo.

Het is moeilijk te voorspellen hoe lang het zal duren om een superintelligente AI te bouwen, maar we weten dat er meer mensen dan ooit aan werken en dat het veld zich in een razend tempo beweegt.
Het kan vele jaren duren of slechts enkele maanden, maar we moeten voorzichtig zijn en nu handelen.

[Lees meer over urgentie](/urgency).

## We nemen het risico niet serieus genoeg

De menselijke geest is geneigd om onder te reageren op risico's die onzichtbaar, langzaam bewegend en moeilijk te begrijpen zijn.
We hebben ook de neiging om exponentiële groei te onderschatten, en we zijn geneigd tot ontkenning wanneer we worden geconfronteerd met bedreigingen voor ons bestaan.

Lees meer over de [psychologie van x-risico](/psychology-of-x-risk).

## AI-bedrijven zijn verwikkeld in een race naar de bodem

OpenAI, DeepMind en Anthropic willen AI veilig ontwikkelen.
Helaas weten ze niet hoe ze dit moeten doen, en ze worden gedwongen door verschillende prikkels om sneller te racen om als eerste AGI te bereiken.
Het [plan](https://openai.com/blog/introducing-superalignment) van OpenAI is om toekomstige AI-systemen te gebruiken om AI af te stemmen. Het probleem hiermee is dat we geen garantie hebben dat we een AI creëren die het afstemmingsprobleem oplost voordat we een AI hebben die catastrofaal gevaarlijk is.
Anthropic [geeft openlijk toe](https://www.anthropic.com/index/core-views-on-ai-safety) dat het nog geen idee heeft hoe het afstemmingsprobleem op te lossen.
DeepMind heeft publiekelijk geen plan aangekondigd om het afstemmingsprobleem op te lossen.

[Dit is waarom we een internationaal verdrag nodig hebben om PauseAI.](/proposal)
