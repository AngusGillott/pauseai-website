---
title: Leer waarom AI-veiligheid belangrijk is
description: Educatieve bronnen (video's, artikelen, boeken) over AI-risico's en AI-afstemming
---

Een van de belangrijkste dingen die je kunt doen om te helpen met AI-afstemming en het existentiële risico (x-risk) dat superintelligentie met zich meebrengt, is erover leren.
Hier zijn enkele bronnen om je op weg te helpen.

## Websites

- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). De startpagina's voor AI-veiligheid. Leer over de risico's, gemeenschappen, evenementen, banen, cursussen, ideeën om de risico's te verminderen en meer!
- [AISafety.dance](https://aisafety.dance). Een leukere, vriendelijkere en interactievere introductie tot de catastrofale risico's van AI!
- [AISafety.world](https://aisafety.world/tiles/). Het volledige AI-veiligheidslandschap met alle organisaties, media, forums, blogs en andere actoren en bronnen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Database van incidenten waarbij AI-systemen schade hebben veroorzaakt.
<!-- [NavigatingAIRisks.ai](https://www.navigatingrisks.ai/). Een blog met verschillende interessante artikelen. - [PauseAI.info](https://pauseai.info). Bekijk de rest van de PauseAI-site hier voor veel gerelateerde informatie en [bronnen](/learn), nuttige [acties](/action), deskundige [citaten](/quotes), korte één-pagina [flyers](PauseAI_flyer.pdf), gerelateerde [veelgestelde vragen](/faq), enz. -->

## Nieuwsbrieven

- [PauseAI Substack](https://pauseai.substack.com/): Onze nieuwsbrief.
- [TransformerNews](https://www.transformernews.ai/) Uitgebreide wekelijkse nieuwsbrief over AI-veiligheid en governance.
- [Maak je geen zorgen over de vaas](https://thezvi.substack.com/): Een nieuwsbrief over AI-veiligheid, rationaliteit en andere onderwerpen.

## Video's

- [Kurzgesagt - A.I. ‐ De Laatste Uitvinding van de Mensheid?](https://www.youtube.com/watch?v=fa8k8IQ1_X0) (20 min). De geschiedenis van AI en een introductie tot het concept van superintelligentie.
- [80k hours - Zou AI de mensheid kunnen uitroeien?](https://youtu.be/qzyEgZwfkKY?si=ief1l2PpkZ7_s6sq) (10 min). Een geweldige introductie tot het probleem, vanuit een nuchtere perspectief.
- [Superintelligente AI zou je zorgen moeten baren...](https://www.youtube.com/watch?v=xBqU1QxCao8) (1 min). De beste superkorte introductie.
- [Kijk niet omhoog - De Documentaire: Het Geval Voor AI Als Een Existentiële Bedreiging](https://www.youtube.com/watch?v=U1eyUjVRir4) (17 min). Krachtige en mooi gemonteerde documentaire over de gevaren van AI, met veel deskundige citaten uit interviews.
- [Landen creëren AI om redenen](https://youtu.be/-9V9cIixPbM?si=L9q6PF2D6h_EBEwF) (10 min). Karikatuur van de race naar een superintelligentie en de gevaren daarvan.
- [Max Tegmark | Ted Talk (2023)](https://www.youtube.com/watch?v=xUNx_PxNHrY) (15 min). AI-capaciteiten verbeteren sneller dan verwacht.
- [Tristan Harris | Nobelprijs Top 2023](https://www.youtube.com/watch?v=6lVBp2XjWsg) (15 min). Toespraak over waarom we onze "paleolithische hersenen moeten omarmen, onze middeleeuwse instellingen moeten upgraden en godachtige technologie moeten binden".
- [Sam Harris | Kunnen we AI bouwen zonder de controle te verliezen?](https://www.youtube.com/watch?v=8nt3edWLgIg) (15 min). Ted talk over de gekke situatie waarin we ons bevinden.
- [Ilya: de AI-wetenschapper die de wereld vormgeeft](https://youtu.be/9iqn1HhFJ6c?si=WnzvpdsPtgCvqAZg) (12 min). Mede-oprichter en voormalig Chief Scientist bij OpenAI legt uit hoe AGI controle zal krijgen over alles en daarom moeten we ze leren om om mensen te geven.
- [De gevaren van Kunstmatige Intelligentie verkennen](https://www.youtube.com/watch?v=sPyu_dTSma0&t=1328s) (25 min). Samenvatting van cybersecurity, biohazard en machtzoekende AI-risico's.
- [Waarom deze top AI-guru denkt dat we in een uitstervingsniveau probleem kunnen zitten | The InnerView](https://youtu.be/YZjmZFDx-pA?si=Y7QUxTaJcuC6LVji) (26 min). Interview met Connor Leahy over AI X-risico's op televisie.
- [Het AI Dilemma](https://www.youtube.com/watch?v=xoVJKj8lcNQ&t=1903s) (1 uur). Presentatie over de gevaren van AI en de race waarin AI-bedrijven vastzitten.
- [Robert Miles' YouTube-video's](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) zijn een geweldige plek om te beginnen met het begrijpen van de meeste fundamenten van AI-afstemming.

## Podcasts

- [Future of Life Institute | Connor Leahy over AI-veiligheid en waarom de wereld fragiel is](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview met Connor over de AI-veiligheidsstrategieën.
- [Lex Fridman | Max Tegmark: Het Pleidooi voor het Stoppen van AI-ontwikkeling](https://youtu.be/VcVfceTsD0A?t=1547). Interview dat in de details van onze huidige gevaarlijke situatie duikt.
- [Sam Harris | Eliezer Yudkowsky: AI, Racen naar de Rand](https://samharris.org/episode/SE60B0CF4B8). Gesprek over de aard van intelligentie, verschillende soorten AI, het afstemprobleem, Is vs Zou, en meer. Een van de vele afleveringen die Making Sense heeft over AI-veiligheid.
- [Connor Leahy, AI Brandalarm](https://youtu.be/pGjyiqJZPJo?t=2510). Praat over de intelligentie-explosie en waarom het de belangrijkste gebeurtenis zou zijn die ooit kan gebeuren.
- [De 80.000 Uren Podcast aanbevolen afleveringen over AI](https://80000hours.org/podcast/on-artificial-intelligence/). Niet 80k uur lang, maar een compilatie van afleveringen van De 80.000 Uren Podcast over AI-veiligheid.
- [Future of Life Institute Podcast afleveringen over AI](https://futureoflife.org/podcast/?_category_browser=ai). Alle afleveringen van de FLI Podcast over de toekomst van Kunstmatige Intelligentie.

Podcasts met leden van PauseAI zijn te vinden in de [media-aandacht](/press) lijst.

## Artikelen

- [De 'Kijk Niet Omhoog' Denkstijl Die Ons Met AI Kan Verdoemen](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (door Max Tegmark)
- [Het Pauzeren van AI-ontwikkelingen Is Niet Genoeg. We Moeten Het Helemaal Stoppen](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (door Eliezer Yudkowsky)
- [Het Pleidooi Voor Het Vertragen Van AI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (door Sigal Samuel)
- [De AI-revolutie: De Weg Naar Superintelligentie](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (door WaitButWhy)
- [Hoe rogue AIs kunnen ontstaan](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (door Yoshua Bengio)
<!-- - [een casual introductie tot AI-doem en afstemming](https://carado.moe/ai-doom.html)
Ik vind het leuk en het feit dat het een lichtere leeservaring biedt, maar ik weet niet zeker of ik het wil opnemen omdat het afstemming alleen definieert als de technische zaken en mensen aanmoedigt om alleen technisch werk te doen -->
- [Redeneren door argumenten tegen het serieus nemen van AI-veiligheid](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (door Yoshua Bengio)

Als je wilt lezen wat journalisten over PauseAI hebben geschreven, bekijk dan de lijst van [media-aandacht](/press).

## Boeken

<!-- - [AI: Onverklaarbaar, Onvoorspelbaar, Oncontroleerbaar](https://www.goodreads.com/book/show/197554072-ai) (Roman Yampolskiy, 2024)
heb nog niet veel goeds gehoord over dit boek -->

- [Oncontroleerbaar: De Bedreiging van Kunstmatige Superintelligentie en de Race om de Wereld te Redden](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Verkrijgbaar voor [gratis](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!
- [De Afgrond: Existentiële Risico's en de Toekomst van de Mensheid](https://www.goodreads.com/en/book/show/50963653) (Toby Ord, 2020)
- [Het Afstemprobleem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Mens Compatibel: Kunstmatige Intelligentie en het Probleem van Controle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leven 3.0: Mens zijn in het Tijdperk van Kunstmatige Intelligentie](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligentie: Paden, Gevaren, Strategieën](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
- [Onze Laatste Uitvinding: Kunstmatige Intelligentie en het Einde van het Menselijke Tijdperk](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)

## Cursussen

- [AGI-veiligheidsfundamentals](https://www.agisafetyfundamentals.com/) (30 uur)
- [CHAI Bibliografie van Aanbevolen Materialen](https://humancompatible.ai/bibliography) (50+ uur)
- [AISafety.training](https://aisafety.training/): Overzicht van trainingsprogramma's, conferenties en andere evenementen

## Organisaties

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) startte de [open brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), geleid door Max Tegmark.
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Conjecture](https://www.conjecture.dev/). Start-up die werkt aan AI-afstemming en AI-beleid, geleid door Connor Leahy.
- [Existential Risk Observatory](https://existentialriskobservatory.org/). Nederlandse organisatie die het publiek informeert over x-risico's en communicatie strategieën bestudeert.
- [Center for AI Safety](https://www.safe.ai/) (CAIS) is een onderzoekscentrum aan de Tsjechische Technische Universiteit in Praag, geleid door
- [Center for Human-Compatible Artificial Intelligence](https://humancompatible.ai/about/) (CHAI), geleid door Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), doet wiskundig onderzoek naar AI-veiligheid, geleid door Eliezer Yudkowsky.
- [Centre for the Governance of AI](https://www.governance.ai/)
- [Institute for AI Policy and Strategy](https://www.iaps.ai/) (IAPS)
- [The AI Policy Institute](https://theaipi.org/)
- [AI Safety Communications Centre](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [The Midas Project](https://www.themidasproject.com/) Bedrijfspressiecampagnes voor AI-veiligheid.
- [The Human Survival Project](https://thehumansurvivalproject.org/)
- [AI Safety World](https://aisafety.world/) Hier is een overzicht van het AI-veiligheidslandschap.

## Als je overtuigd bent en actie wilt ondernemen

Er zijn veel [dingen die je kunt doen](/action).
Een brief schrijven, naar een protest gaan, wat geld doneren of lid worden van een gemeenschap is niet zo moeilijk!
En deze acties hebben een echte impact.
Zelfs wanneer je geconfronteerd wordt met het einde van de wereld, kan er nog steeds hoop zijn en zeer lonend werk te doen.

## Of als je nog steeds niet helemaal zeker bent

Leren over de [psychologie van x-risk](/psychology-of-x-risk) kan je helpen.
